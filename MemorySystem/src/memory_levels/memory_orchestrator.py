"""
Memory Orchestrator –¥–ª—è AIRI Memory System
–ì–ª–∞–≤–Ω—ã–π –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–æ—Ä –≤—Å–µ—Ö —É—Ä–æ–≤–Ω–µ–π –ø–∞–º—è—Ç–∏
"""

import asyncio
import time
import uuid
import os
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional, Tuple, Set
from dataclasses import dataclass
import logging
from ..utils.validation import validate_importance, validate_confidence, validate_metadata_values
# –ò–º–ø–æ—Ä—Ç –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ - –ü–†–ê–í–ò–õ–¨–ù–´–ï –û–¢–ù–û–°–ò–¢–ï–õ–¨–ù–´–ï –ò–ú–ü–û–†–¢–´
from ..monitoring.metrics import inc, record_event, merge_counters, set_gauge
from ..monitoring.performance_tracker import track_operation_performance
from ..monitoring.search_metrics import record_search_metrics

try:
    from .sensor_buffer import SensorBuffer, SensorType, SensorData
    from .working_memory import WorkingMemoryManager, WorkingMemoryItem
    from .short_term_memory import ShortTermMemoryManager, ShortTermMemoryItem
    from .episodic_memory import EpisodicMemoryManager, EpisodicMemoryItem
    from .semantic_memory import SemanticMemoryManager, SemanticMemoryItem
    from .graph_memory_sqlite import GraphMemoryManagerSQLite as GraphMemoryManager, GraphNode, GraphEdge
    from .procedural_memory import ProceduralMemoryManager, ProceduralMemoryItem
    from .memory_consolidator import MemoryConsolidator
except ImportError:
    from sensor_buffer import SensorBuffer, SensorType, SensorData
    from working_memory import WorkingMemoryManager, WorkingMemoryItem
    from short_term_memory import ShortTermMemoryManager, ShortTermMemoryItem
    from episodic_memory import EpisodicMemoryManager, EpisodicMemoryItem
    from semantic_memory import SemanticMemoryManager, SemanticMemoryItem
    from graph_memory_sqlite import GraphMemoryManagerSQLite as GraphMemoryManager, GraphNode, GraphEdge
    from procedural_memory import ProceduralMemoryManager, ProceduralMemoryItem
    from memory_consolidator import MemoryConsolidator
try:
    from .proactive_goals_timer import ProactiveGoalsTimer, ProactiveGoal, GoalTriggerType, GoalActionType, GoalStatus
except ImportError:
    from proactive_goals_timer import ProactiveGoalsTimer, ProactiveGoal, GoalTriggerType, GoalActionType, GoalStatus
try:
    from ..extractors.fact_extractor import FactExtractor
    from ..classifiers.event_classifier import EventClassifier
    from ..prioritizers.memory_prioritizer import compute_priority
    from ..analyzers.relationship_analyzer import naive_relationships, advanced_relationships
    from ..emotion_analysis_service import get_emotion_service
    from ..search.entity_extractor import EntityExtractor
except ImportError:
    from extractors.fact_extractor import FactExtractor
    from classifiers.event_classifier import EventClassifier
    from prioritizers.memory_prioritizer import compute_priority
    from analyzers.relationship_analyzer import naive_relationships, advanced_relationships
    from emotion_analysis_service import get_emotion_service
    from search.entity_extractor import EntityExtractor

logger = logging.getLogger(__name__)

@dataclass
class MemoryQuery:
    """–ó–∞–ø—Ä–æ—Å –∫ —Å–∏—Å—Ç–µ–º–µ –ø–∞–º—è—Ç–∏"""
    query: str
    user_id: str
    memory_levels: Optional[List[str]] = None
    limit: int = 10
    offset: int = 0
    include_emotions: bool = True
    include_relationships: bool = True
    track_interlevel_references: bool = True  # –û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –º–µ–∂—É—Ä–æ–≤–Ω–µ–≤—ã—Ö —Å—Å—ã–ª–æ–∫

@dataclass
class MemoryResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ–∏—Å–∫–∞ –≤ –ø–∞–º—è—Ç–∏"""
    level: str
    items: List[Any]
    relevance_scores: List[float]
    total_found: int

@dataclass
class UnifiedMemoryResult:
    """–£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ–∏—Å–∫–∞"""
    query: str
    user_id: str
    results: Dict[str, MemoryResult]
    total_items: int
    processing_time: float
    recommendations: Optional[List[str]] = None

class MemoryOrchestrator:
    """–û—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä —Å–∏—Å—Ç–µ–º—ã –ø–∞–º—è—Ç–∏"""
    
    def __init__(self, chromadb_path: Optional[str] = None):
        # Resolve data directories from ENV for offline-install layout
        data_root = os.getenv("AIRI_DATA_DIR", "./data")
        chroma_base = os.getenv("CHROMADB_DIR", os.path.join(data_root, "chroma_db"))
        base = chromadb_path or chroma_base
        self.chromadb_path = base
        
        # –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –°–æ–∑–¥–∞–µ–º –µ–¥–∏–Ω—ã–π LLM Provider –¥–ª—è –≤—Å–µ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        # –≠—Ç–æ —É—Å—Ç—Ä–∞–Ω—è–µ—Ç –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—É—é –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é –∏ —É—Å–∫–æ—Ä—è–µ—Ç —Ö–æ–ª–æ–¥–Ω—ã–π —Å—Ç–∞—Ä—Ç
        try:
            try:
                from ..providers.lm_studio_provider import LMStudioProvider
            except ImportError:
                from providers.lm_studio_provider import LMStudioProvider
            self.llm_provider = LMStudioProvider("config/lm_studio_config.yaml")
            logger.info("Shared LLM Provider initialized")
        except Exception as e:
            logger.warning(f"Failed to initialize shared LLM Provider: {e}")
            self.llm_provider = None
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–µ–Ω–µ–¥–∂–µ—Ä—ã –≤—Å–µ—Ö —É—Ä–æ–≤–Ω–µ–π –ø–∞–º—è—Ç–∏
        # –£—Ä–æ–≤–µ–Ω—å 1: Sensor Buffer - –∫–æ–ª—å—Ü–µ–≤–æ–π –±—É—Ñ–µ—Ä –¥–ª—è –ø–æ—Ç–æ–∫–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        self.sensor_buffer = SensorBuffer()
        
        # –£—Ä–æ–≤–Ω–∏ 2-7: –û—Å–Ω–æ–≤–Ω—ã–µ —É—Ä–æ–≤–Ω–∏ –ø–∞–º—è—Ç–∏
        try:
            self.working_memory = WorkingMemoryManager(base)
            self.short_term_memory = ShortTermMemoryManager(base)
            self.episodic_memory = EpisodicMemoryManager(base)
            self.semantic_memory = SemanticMemoryManager(base)
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –µ–¥–∏–Ω—É—é –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≥—Ä–∞—Ñ–∞
            graph_db_path = os.path.join(base, "memory_system.db")
            self.graph_memory = GraphMemoryManager(graph_db_path)
            self.procedural_memory = ProceduralMemoryManager(base)
            # –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –ü–µ—Ä–µ–¥–∞–µ–º –µ–¥–∏–Ω—ã–π LLM Provider –≤—Å–µ–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º
            self.fact_extractor = FactExtractor(self.llm_provider)
            self.event_classifier = EventClassifier()
            self.entity_extractor = EntityExtractor(self.llm_provider)
            logger.info("All memory managers initialized successfully with shared LLM Provider")
        except Exception as e:
            logger.error(f"Error initializing memory managers: {e}")
            raise
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ–∏—Å–∫–∞ (8-—É—Ä–æ–≤–Ω–µ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –ø–∞–º—è—Ç–∏)
        self.search_weights = {
            "sensor": 0.35,      # –°–∞–º—ã–π –≤—ã—Å–æ–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –¥–ª—è —Ç–µ–∫—É—â–∏—Ö —Å–µ–Ω—Å–æ—Ä–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            "working": 0.25,     # –í—ã—Å–æ–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –¥–ª—è –∞–∫—Ç–∏–≤–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            "short_term": 0.2,   # –í—ã—Å–æ–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –¥–ª—è –Ω–µ–¥–∞–≤–Ω–∏—Ö —Å–æ–±—ã—Ç–∏–π
            "episodic": 0.1,     # –°—Ä–µ–¥–Ω–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –¥–ª—è –∑–Ω–∞—á–∏–º—ã—Ö —Å–æ–±—ã—Ç–∏–π
            "semantic": 0.05,    # –ù–∏–∑–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –¥–ª—è –∑–Ω–∞–Ω–∏–π
            "fts5": 0.03,        # –ù–∏–∑–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –¥–ª—è –∫–ª—é—á–µ–≤–æ–≥–æ –ø–æ–∏—Å–∫–∞
            "graph": 0.03,       # –ù–∏–∑–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –¥–ª—è —Å–≤—è–∑–µ–π
            "procedural": 0.02   # –ù–∏–∑–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –¥–ª—è –Ω–∞–≤—ã–∫–æ–≤
        }
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–µ—Ç—Ä–∏–∫ –¥–ª—è —Ñ–æ–Ω–æ–≤—ã—Ö –∑–∞–¥–∞—á
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–µ—Ç—Ä–∏–∫ —Ñ–æ–Ω–æ–≤—ã—Ö –∑–∞–¥–∞—á - –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û!
        try:
            inc("background_tasks_started", 0)  # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å—á–µ—Ç—á–∏–∫–∞
            inc("background_tasks_completed", 0)
            inc("background_tasks_failed", 0)
            set_gauge("background_tasks_queue_size", 0)
            logger.info("‚úÖ Background task metrics initialized successfully")
        except Exception as e:
            logger.error(f"üö® CRITICAL: Failed to initialize background task metrics: {e}")
            logger.error("üö® Monitoring fallback: using in-memory metrics")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–Ω—Å–æ–ª–∏–¥–∞—Ç–æ—Ä–∞ –ø–∞–º—è—Ç–∏
        self.consolidator = MemoryConsolidator(self)
        logger.info("Memory consolidator initialized")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–∞–∫—Ç–∏–≤–Ω–æ–≥–æ —Ç–∞–π–º–µ—Ä–∞ —Ü–µ–ª–µ–π
        self.proactive_timer = ProactiveGoalsTimer(self)
        logger.info("Proactive goals timer initialized")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –ø–∞–º—è—Ç–∏
        try:
            # –ò–º–ø–æ—Ä—Ç –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –ø–∞–º—è—Ç–∏ - –ü–†–ê–í–ò–õ–¨–ù–´–ï –û–¢–ù–û–°–ò–¢–ï–õ–¨–ù–´–ï –ò–ú–ü–û–†–¢–´
            from ..monitoring.memory_monitor import get_memory_monitor
            from ..monitoring.memory_cleanup import get_memory_cleanup
            
            self.memory_monitor = get_memory_monitor()
            self.memory_cleanup = get_memory_cleanup()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É—Å–ø–µ—à–Ω–æ—Å—Ç—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
            if self.memory_monitor is None:
                logger.warning("‚ö†Ô∏è Memory monitor is None - monitoring may be disabled")
            else:
                logger.info("‚úÖ Memory monitor initialized successfully")
                
            if self.memory_cleanup is None:
                logger.warning("‚ö†Ô∏è Memory cleanup is None - cleanup may be disabled")
            else:
                logger.info("‚úÖ Memory cleanup initialized successfully")
            
            # –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
            self.memory_monitor.register_component(
                "sensor_buffer",
                self._get_sensor_buffer_stats,
                None  # cleanup —Ñ—É–Ω–∫—Ü–∏–∏ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –≤ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–µ
            )
            self.memory_monitor.register_component(
                "memory_orchestrator",
                self._get_orchestrator_stats,
                None  # cleanup —Ñ—É–Ω–∫—Ü–∏–∏ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –≤ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–µ
            )
            
            logger.info("Memory monitoring initialized")
        except Exception as e:
            logger.warning(f"Failed to initialize memory monitoring: {e}")
            self.memory_monitor = None
            self.memory_cleanup = None
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –û–¢–ö–õ–Æ–ß–ï–ù–ê
        # try:
        #     try:
        #         from ..recovery.auto_recovery import get_auto_recovery
        #         from ..recovery.recovery_handlers import get_recovery_handlers
        #     except ImportError:
        #         from recovery.auto_recovery import get_auto_recovery
        #         from recovery.recovery_handlers import get_recovery_handlers
        #     
        #     self.auto_recovery = get_auto_recovery()
        #     # –ü–µ—Ä–µ–¥–∞–µ–º self (memory_orchestrator) –≤ recovery_handlers
        #     self.recovery_handlers = get_recovery_handlers(memory_orchestrator=self)
        
        self.auto_recovery = None
        self.recovery_handlers = None
        
        # –†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –¥–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –û–¢–ö–õ–Æ–ß–ï–ù–ê
        # # –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –¥–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è
        # self.auto_recovery.register_component(
        #     "memory_orchestrator",
        #     self.recovery_handlers.check_memory_orchestrator_health,
        #     self.recovery_handlers.recover_memory_orchestrator,
        #     is_critical=True,
        #     check_interval=120,  # 2 –º–∏–Ω—É—Ç—ã –≤–º–µ—Å—Ç–æ 30 —Å–µ–∫—É–Ω–¥
        #     max_failures=5       # –ë–æ–ª—å—à–µ –ø–æ–ø—ã—Ç–æ–∫
        # )
        # 
        # self.auto_recovery.register_component(
        #     "sensor_buffer",
        #     self.recovery_handlers.check_sensor_buffer_health,
        #     self.recovery_handlers.recover_sensor_buffer,
        #     is_critical=False,
        #     check_interval=60,
        #     max_failures=5
        # )
        # 
        # self.auto_recovery.register_component(
        #     "memory_cache",
        #     self.recovery_handlers.check_memory_cache_health,
        #     self.recovery_handlers.recover_memory_cache,
        #     is_critical=False,
        #     check_interval=120,
        #     max_failures=3
        # )
        # 
        # self.auto_recovery.register_component(
        #     "sqlite_cache",
        #     self.recovery_handlers.check_sqlite_cache_health,
        #     self.recovery_handlers.recover_sqlite_cache,
        #     is_critical=True,
        #     check_interval=60,
        #     max_failures=3
        # )
        # 
        # self.auto_recovery.register_component(
        #     "lm_provider",
        #     self.recovery_handlers.check_lm_provider_health,
        #     self.recovery_handlers.recover_lm_provider,
        #     is_critical=True,
        #     check_interval=180,  # 3 –º–∏–Ω—É—Ç—ã –≤–º–µ—Å—Ç–æ 30 —Å–µ–∫—É–Ω–¥
        #     max_failures=3
        # )
        
        logger.info("Auto recovery –û–¢–ö–õ–Æ–ß–ï–ù")
        # except Exception as e:
        #     logger.warning(f"Failed to initialize auto recovery: {e}")
        #     self.auto_recovery = None
        #     self.recovery_handlers = None
        
        # –°—á–µ—Ç—á–∏–∫–∏ –¥–ª—è —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ–π —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ (thread-safe)
        self._summary_counters = {}  # user_id -> count
        self._summary_counters_lock = asyncio.Lock()  # Thread-safe –±–ª–æ–∫–∏—Ä–æ–≤–∫–∞
        self._summary_threshold = 10  # –∫–∞–∂–¥—ã–µ 10 —Å–æ–æ–±—â–µ–Ω–∏–π
        
        # –û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –∞–∫—Ç–∏–≤–Ω—ã—Ö summary –∑–∞–¥–∞—á –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è
        self._summary_tasks = {}  # user_id -> asyncio.Task
        self._summary_task_locks = {}  # user_id -> asyncio.Lock
        
        # –û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ —Ñ–æ–Ω–æ–≤—ã—Ö –∑–∞–¥–∞—á –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
        self._background_tasks: Set[asyncio.Task] = set()
        self._shutdown_event = asyncio.Event()
    
    def _add_background_task(self, task: asyncio.Task) -> None:
        """–î–æ–±–∞–≤–ª—è–µ—Ç —Ñ–æ–Ω–æ–≤—É—é –∑–∞–¥–∞—á—É –≤ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ"""
        self._background_tasks.add(task)
        task.add_done_callback(self._background_tasks.discard)
    
    async def _queue_for_retry(self, task_type: str, task_data: dict):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∑–∞–¥–∞—á–∏ –≤ –æ—á–µ—Ä–µ–¥—å –¥–ª—è –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        try:
            if not hasattr(self, '_retry_queue'):
                self._retry_queue = []
            
            retry_task = {
                "task_type": task_type,
                "task_data": task_data,
                "created_at": time.time(),
                "next_retry": time.time() + 60  # –ü–æ–≤—Ç–æ—Ä —á–µ—Ä–µ–∑ 1 –º–∏–Ω—É—Ç—É
            }
            
            self._retry_queue.append(retry_task)
            logger.info(f"Task queued for retry: {task_type}, queue size: {len(self._retry_queue)}")
            
        except Exception as e:
            logger.error(f"Failed to queue task for retry: {e}")
            raise
    
    async def shutdown(self) -> None:
        """–ö–æ—Ä—Ä–µ–∫—Ç–Ω–æ–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä–∞"""
        logger.info("Shutting down MemoryOrchestrator...")
        
        # –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø—Ä–æ–∞–∫—Ç–∏–≤–Ω—ã–π —Ç–∞–π–º–µ—Ä
        if hasattr(self, 'proactive_timer'):
            await self.proactive_timer.stop()
            logger.info("Proactive goals timer stopped")
        
        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ñ–ª–∞–≥ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
        self._shutdown_event.set()
        
        # –û—Ç–º–µ–Ω—è–µ–º –≤—Å–µ —Ñ–æ–Ω–æ–≤—ã–µ –∑–∞–¥–∞—á–∏
        if self._background_tasks:
            logger.info(f"Cancelling {len(self._background_tasks)} background tasks...")
            for task in self._background_tasks.copy():
                if not task.done():
                    task.cancel()
            
            # –ñ–¥–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –≤—Å–µ—Ö –∑–∞–¥–∞—á
            if self._background_tasks:
                await asyncio.gather(*self._background_tasks, return_exceptions=True)
        
        logger.info("MemoryOrchestrator shutdown complete")
        
    async def add_memory(self, content: str, user_id: str, 
                        memory_type: str = "general", importance: float = 0.5,
                        emotion_data: Optional[Dict[str, Any]] = None,
                        context: Optional[str] = None,
                        participants: Optional[List[str]] = None,
                        location: Optional[str] = None) -> Dict[str, str]:
        """
        –î–æ–±–∞–≤–∏—Ç—å –ø–∞–º—è—Ç—å –Ω–∞ –≤—Å–µ —É—Ä–æ–≤–Ω–∏
        
        Args:
            content: –°–æ–¥–µ—Ä–∂–∏–º–æ–µ –ø–∞–º—è—Ç–∏
            user_id: ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            memory_type: –¢–∏–ø –ø–∞–º—è—Ç–∏ (conversation, event, knowledge, skill, etc.)
            importance: –í–∞–∂–Ω–æ—Å—Ç—å (0.0-1.0) - –≤–∞–ª–∏–¥–∏—Ä—É–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏
            emotion_data: –≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            context: –ö–æ–Ω—Ç–µ–∫—Å—Ç
            participants: –£—á–∞—Å—Ç–Ω–∏–∫–∏
            location: –ú–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏–µ
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å ID —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –Ω–∞ –∫–∞–∂–¥–æ–º —É—Ä–æ–≤–Ω–µ
        """
        try:
            start_time = time.time()
            results = {}
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è –≤—Ö–æ–¥–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
            importance = validate_importance(importance)
            
            logger.info(f"AddMemory: user={user_id} type={memory_type} importance={importance:.2f}")
            
            # –£–†–û–í–ï–ù–¨ 1: Sensor Buffer - –¥–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ –∫–æ–ª—å—Ü–µ–≤–æ–π –±—É—Ñ–µ—Ä
            try:
                sensor_id = self.sensor_buffer.add_sensor_data(
                    sensor_type=SensorType.TEXT,
                    data=content,
                    user_id=user_id,
                    metadata={
                        "memory_type": memory_type,
                        "importance": importance,
                        "context": context,
                        "participants": participants,
                        "location": location
                    },
                    text_content=content
                )
                results["sensor"] = sensor_id
                logger.debug(f"–î–æ–±–∞–≤–ª–µ–Ω–æ –≤ Sensor Buffer: {sensor_id}")
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –≤ Sensor Buffer: {e}")
                results["sensor"] = None
            from ..monitoring.metrics import time_block
            with time_block("memory_add_emotion_analysis"):
                # 0. Auto emotion analysis (optional)
                if emotion_data is None and os.getenv("EMOTION_AUTO", "1") in ("1", "true", "True"):
                    try:
                        svc = get_emotion_service()
                        res = await svc.analyze_text_emotions(content, include_validation=True)
                        emotion_data = {
                            "primary_emotion": res.primary_emotion,
                            "primary_confidence": res.primary_confidence,
                            "secondary_emotion": res.secondary_emotion,
                            "secondary_confidence": res.secondary_confidence,
                            "tertiary_emotion": res.tertiary_emotion,
                            "tertiary_confidence": res.tertiary_confidence,
                            "consistency": res.consistency,
                            "dominant_source": res.dominant_source,
                            "validation_applied": res.validation_applied,
                        }
                        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –∞–Ω–∞–ª–∏–∑–∞ —ç–º–æ—Ü–∏–π - –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û!
                        try:
                            inc("emotion_analysis_total")
                            logger.debug("‚úÖ Emotion analysis metrics updated successfully")
                        except Exception as e:
                            logger.error(f"üö® CRITICAL: Failed to update emotion analysis metrics: {e}")
                            logger.error("üö® Monitoring fallback: using in-memory metrics")
                    except Exception as e:
                        logger.warning(f"Auto emotion analysis failed: {e}")

                # 1. Working Memory - –≤—Å–µ–≥–¥–∞ –¥–æ–±–∞–≤–ª—è–µ–º
                wm_id = await self.working_memory.add_memory(
                    content=content,
                    user_id=user_id,
                    importance=importance,
                    confidence=validate_confidence(min(1.0, max(0.0, importance * 0.8 + 0.2))),
                    context=context,
                    emotion_data=emotion_data
                )
                results["working"] = wm_id
                # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ Working Memory - –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û!
                try:
                    inc("memory_add_level_working")
                    logger.debug("‚úÖ Working memory metrics updated successfully")
                except Exception as _e:
                    logger.error(f"üö® CRITICAL: Failed to update working memory metrics: {_e}")
                    logger.error("üö® Monitoring fallback: using in-memory metrics")
            
            # 2. Short-term Memory - –¥–æ–±–∞–≤–ª—è–µ–º –µ—Å–ª–∏ –≤–∞–∂–Ω–æ—Å—Ç—å > 0.3 (—Å –∞–≤—Ç–æ-–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–µ–π —Å–æ–±—ã—Ç–∏—è)
            if importance > 0.3:
                try:
                    auto_type = await self.event_classifier.classify(content)
                except Exception:
                    auto_type = memory_type
                stm_id = await self.short_term_memory.add_event(
                    content=content,
                    user_id=user_id,
                    importance=importance,
                    confidence=validate_confidence(min(1.0, max(0.0, importance * 0.8 + 0.2))),
                    event_type=auto_type or memory_type,
                    location=location,
                    participants=participants or [],
                    emotion_data=emotion_data
                )
                results["short_term"] = stm_id
                # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ Short-term Memory - –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û!
                try:
                    inc("memory_add_level_short_term")
                    logger.debug("‚úÖ Short-term memory metrics updated successfully")
                except Exception as _e:
                    logger.error(f"üö® CRITICAL: Failed to update short-term memory metrics: {_e}")
                    logger.error("üö® Monitoring fallback: using in-memory metrics")
            
            # 3. Episodic Memory - –¥–æ–±–∞–≤–ª—è–µ–º –µ—Å–ª–∏ –≤–∞–∂–Ω–æ—Å—Ç—å > 0.5
            if importance > 0.5:
                with time_block("memory_add_episodic"):
                    ep_id = await self.episodic_memory.add_experience(
                        content=content,
                        user_id=user_id,
                        importance=importance,
                        confidence=validate_confidence(min(1.0, max(0.0, importance * 0.8 + 0.2))),
                        event_type=memory_type,
                        location=location,
                        participants=participants or [],
                        emotion_data=emotion_data,
                        significance=importance,
                        vividness=0.7 if emotion_data else 0.5,
                        context=context
                    )
                    results["episodic"] = ep_id
                    try:
                        inc("memory_add_level_episodic")
                    except Exception as _e:
                        logger.warning(f"metrics inc failed (episodic): {_e}")
            
            # 4. Semantic Memory - –¥–æ–±–∞–≤–ª—è–µ–º –µ—Å–ª–∏ —ç—Ç–æ –∑–Ω–∞–Ω–∏–µ, –∞ —Ç–∞–∫–∂–µ –∏–∑–≤–ª–µ–∫–∞–µ–º —Ñ–∞–∫—Ç—ã –ø—Ä–∏ –≤—ã—Å–æ–∫–æ–π –≤–∞–∂–Ω–æ—Å—Ç–∏
            if memory_type in ["knowledge", "fact", "concept", "definition"]:
                sm_id = await self.semantic_memory.add_knowledge(
                    content=content,
                    user_id=user_id,
                    knowledge_type=memory_type,
                    category="general",
                    confidence=validate_confidence(min(1.0, max(0.0, importance * 0.8 + 0.2))),
                    importance=importance
                )
                results["semantic"] = [sm_id]
                try:
                    inc("memory_add_level_semantic")
                except Exception as _e:
                    logger.warning(f"metrics inc failed (semantic-extracted): {_e}")
            elif importance >= 0.5:
                # –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –§–∞–∫—Ç-—ç–∫—Å—Ç—Ä–∞–∫—Ü–∏—è —á–µ—Ä–µ–∑ –µ–¥–∏–Ω—ã–π LLM –≤—ã–∑–æ–≤
                # –§–∞–∫—Ç—ã —É–∂–µ –∏–∑–≤–ª–µ—á–µ–Ω—ã –≤ _process_memory_unified, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏—Ö
                with time_block("memory_add_semantic_unified"):
                    unified_result = await self._process_memory_unified(content, user_id, importance)
                    facts = unified_result.get("facts", [])
                    logger.info(f"DEBUG: unified_result keys: {list(unified_result.keys())}")
                    logger.info(f"DEBUG: facts from unified_result: {facts}")
                    logger.info(f"DEBUG: facts type: {type(facts)}, len: {len(facts) if facts else 'None'}")
                
                if facts:
                    logger.info(f"Processing {len(facts)} facts from unified result")
                    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ñ–∞–∫—Ç—ã –∫–∞–∫ semantic memories —Å –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–µ–π
                    processed_facts = set()  # –î–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
                    for fact in facts:
                        if isinstance(fact, list) and len(fact) >= 3:
                            subject, relation, object_ = fact[0], fact[1], fact[2]
                            fact_content = f"{subject} {relation} {object_}"
                            
                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
                            if fact_content in processed_facts:
                                logger.debug(f"Skipping duplicate fact: {fact_content}")
                                continue
                            
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –≤ –±–∞–∑–µ
                        existing = await self.semantic_memory.search_knowledge(
                            user_id=user_id,
                            query=fact_content,
                            limit=1,
                            min_confidence=0.25  # –£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ—Ä–æ–≥ confidence
                        )
                        
                        logger.info(f"DEDUPLICATION CHECK: fact='{fact_content}', existing_count={len(existing) if existing else 0}")
                        
                        if existing and len(existing) > 0:
                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º distance - –µ—Å–ª–∏ 0.0, —Ç–æ —ç—Ç–æ —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
                            first_result = existing[0]
                            distance = getattr(first_result, 'distance', None)
                            logger.info(f"DEDUPLICATION CHECK: distance={distance}, threshold=0.1, should_skip={distance is not None and distance <= 0.1}")
                            if distance is not None and distance <= 0.1:  # –û—á–µ–Ω—å –±–ª–∏–∑–∫–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
                                logger.info(f"SKIPPING DUPLICATE FACT: distance={distance}, fact='{fact_content}'")
                                continue
                            else:
                                logger.info(f"KEEPING FACT: distance={distance} > 0.1, fact='{fact_content}'")
                        
                        # –î–æ–±–∞–≤–ª—è–µ–º —Ñ–∞–∫—Ç —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –æ–Ω –Ω–µ –¥—É–±–ª–∏–∫–∞—Ç
                        processed_facts.add(fact_content)
                        sm_id = await self.semantic_memory.add_knowledge(
                            content=fact_content,
                            user_id=user_id,
                            importance=importance * 0.8,  # –§–∞–∫—Ç—ã –Ω–µ–º–Ω–æ–≥–æ –º–µ–Ω–µ–µ –≤–∞–∂–Ω—ã
                            knowledge_type="fact",
                            source="unified_processing"
                        )
                        logger.info(f"Added fact as semantic memory: {sm_id}")
                else:
                    logger.info(f"No facts extracted for user {user_id}")
                
                try:
                    inc("memory_add_level_semantic")
                except Exception as e:
                    logger.warning(f"Failed to update semantic metrics: {e}")
            
            # 5. Graph Memory - —Å–æ–∑–¥–∞–µ–º —É–∑–ª—ã –¥–ª—è –≤–∞–∂–Ω—ã—Ö –∫–æ–Ω—Ü–µ–ø—Ü–∏–π –∏ —Å–≤—è–∑—ã–≤–∞–µ–º –∏—Ö
            if importance >= 0.5:
                logger.info(f"Graph processing triggered for importance={importance}")
                # –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–∂–µ –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ –∏–∑ –µ–¥–∏–Ω–æ–≥–æ –≤—ã–∑–æ–≤–∞
                # –ï—Å–ª–∏ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ –µ—â–µ –Ω–µ –∏–∑–≤–ª–µ—á–µ–Ω—ã, –∏–∑–≤–ª–µ–∫–∞–µ–º –∏—Ö
                if 'concepts' not in locals():
                    logger.info("Extracting concepts for graph processing")
                    try:
                        concepts, entity_types = await self._extract_concepts(content, user_id, importance)
                        logger.info(f"Extracted {len(concepts)} concepts: {concepts}")
                        logger.info(f"Entity types: {entity_types}")
                    except Exception as e:
                        logger.error(f"Error extracting concepts: {e}")
                        concepts = []
                        entity_types = {}
                else:
                    concepts = []  # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º concepts –µ—Å–ª–∏ –Ω–µ –∏–∑–≤–ª–µ—á–µ–Ω—ã
                    logger.info(f"Using existing concepts: {concepts}")
                
                logger.info(f"Final concepts for graph processing: {concepts}")
                if concepts:
                    logger.info(f"Creating {len(concepts)} graph nodes")
                    added_node_ids = []
                    for concept in concepts:
                        try:
                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —É–∑–ª–∞ –ø–µ—Ä–µ–¥ —Å–æ–∑–¥–∞–Ω–∏–µ–º
                            existing_nodes = await self.graph_memory.search_nodes(
                                user_id=user_id,
                                query=concept,
                                node_type=entity_types.get(concept, "concept"),
                                limit=1
                            )
                            
                            if existing_nodes:
                                # –£–∑–µ–ª —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ ID
                                existing_node = existing_nodes[0]
                                node_id = existing_node.id
                                logger.info(f"Using existing graph node: {concept} -> {node_id}")
                            else:
                                # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π —É–∑–µ–ª
                                node_type = entity_types.get(concept, "concept")
                                node_id = await self.graph_memory.add_node(
                                    name=concept,
                                    user_id=user_id,
                                    node_type=node_type,
                                    importance=importance
                                )
                                logger.info(f"Created new graph node: {concept} -> {node_id}")
                            
                            added_node_ids.append(node_id)
                        except Exception as e:
                            logger.error(f"Error creating graph node for concept '{concept}': {e}")
                else:
                    logger.warning("No concepts to create graph nodes")
                    added_node_ids = []
                
                # —Å–≤—è–∑—ã–≤–∞–µ–º –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ –º–µ–∂–¥—É —Å–æ–±–æ–π –±–∞–∑–æ–≤–æ–π —Å–≤—è–∑—å—é 'related'
                for i in range(len(added_node_ids)):
                    for j in range(i + 1, len(added_node_ids)):
                        try:
                            await self.graph_memory.add_edge(
                                source_id=added_node_ids[i],
                                target_id=added_node_ids[j],
                                user_id=user_id,
                                relationship_type="related",
                                strength=min(1.0, importance)
                            )
                        except Exception as e:
                            logger.warning(f"Failed to update relationship analysis metrics: {e}")
                # –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –ê–Ω–∞–ª–∏–∑ –æ—Ç–Ω–æ—à–µ–Ω–∏–π - –ê–°–ò–ù–•–†–û–ù–ù–û (–¥–ª—è –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è)
                # –û—Å–Ω–æ–≤–Ω—ã–µ —É–∑–ª—ã —É–∂–µ —Å–æ–∑–¥–∞–Ω—ã, LLM –ø–æ–ª—É—á–∏—Ç –ø–æ–ª–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
                task = asyncio.create_task(self._analyze_relationships_async(content, user_id, added_node_ids, importance))
                self._add_background_task(task)
                # –î–æ–±–∞–≤–ª—è–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –∏—Å–∫–ª—é—á–µ–Ω–∏–π –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –∑–∞–≤–∏—Å–∞–Ω–∏—è
                task.add_done_callback(lambda t: logger.debug(f"Relationship analysis task completed: {t.exception() if t.exception() else 'success'}"))
                try:
                    inc("background_tasks_started")
                except Exception as e:
                    logger.warning(f"Failed to update background tasks metrics: {e}")
                logger.info(f"Relationship analysis queued for async processing: user={user_id} nodes={len(added_node_ids)}")
                if added_node_ids:
                    results["graph"] = added_node_ids
                    try:
                        inc("memory_add_level_graph")
                    except Exception as _e:
                        logger.warning(f"metrics inc failed (graph): {_e}")
            else:
                logger.info(f"Graph processing skipped for importance={importance} (threshold: 0.5)")
                added_node_ids = []
            
            # 6. Procedural Memory - –¥–æ–±–∞–≤–ª—è–µ–º –µ—Å–ª–∏ —ç—Ç–æ –Ω–∞–≤—ã–∫
            if memory_type in ["skill", "procedure", "algorithm", "method"]:
                pm_id = await self.procedural_memory.add_skill(
                    name=content.split(":")[0] if ":" in content else content[:50],
                    description=content,
                    user_id=user_id,
                    skill_type="cognitive",
                    difficulty=importance,
                    importance=importance
                )
                results["procedural"] = pm_id
                try:
                    inc("memory_add_level_procedural")
                except Exception as _e:
                    logger.warning(f"metrics inc failed (procedural): {_e}")
            
            processing_time = time.time() - start_time
            logger.info(f"AddMemory: stored={results.keys()} duration={processing_time:.2f}s")
            try:
                record_event("add_memory", {
                    "user_id": user_id,
                    "type": memory_type,
                    "importance": importance,
                    "levels": list(results.keys()),
                    "duration": processing_time,
                })
            except Exception as _e:
                logger.warning(f"record_event(add_memory) failed: {_e}")
            
            # –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –ó–∞–ø—É—Å–∫–∞–µ–º —Ñ–æ–Ω–æ–≤—ã–µ –∑–∞–¥–∞—á–∏ (–Ω–µ –±–ª–æ–∫–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç)
            asyncio.create_task(self._background_post_processing(user_id, content, results, memory_type, importance))
            logger.info(f"Background post-processing started for user {user_id}")
            
            # –ö–†–ò–¢–ò–ß–ù–û: –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å —Å–æ–∑–¥–∞–Ω–∏—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ (–∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –≤ —Ñ–æ–Ω–µ)
            # –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –ò—Å–ø–æ–ª—å–∑—É–µ–º per-user lock –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è race conditions
            try:
                # –°–æ–∑–¥–∞–µ–º lock –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
                if user_id not in self._summary_task_locks:
                    self._summary_task_locks[user_id] = asyncio.Lock()
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏ —Å–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á—É –ø–æ–¥ lock
                async def _safe_create_summary_task():
                    async with self._summary_task_locks[user_id]:
                        # –î–≤–æ–π–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–¥ lock
                        if user_id not in self._summary_tasks or self._summary_tasks[user_id].done():
                            # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é –∑–∞–¥–∞—á—É —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –ø—Ä–µ–¥—ã–¥—É—â–∞—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞
                            task = asyncio.create_task(self._check_and_create_summary_async(user_id))
                            self._summary_tasks[user_id] = task
                            logger.debug(f"Summary creation task started for user {user_id}")
                        else:
                            logger.debug(f"Summary creation task already running for user {user_id}")
                
                # –ó–∞–ø—É—Å–∫–∞–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É –≤ —Ñ–æ–Ω–µ
                asyncio.create_task(_safe_create_summary_task())
            except Exception as e:
                logger.warning(f"Failed to start summary creation for user {user_id}: {e}")
            
            # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            duration_ms = (time.time() - start_time) * 1000
            track_operation_performance(
                operation_type="add_memory",
                operation_name="memory_orchestrator_add",
                user_id=user_id,
                duration_ms=duration_ms,
                success=True,
                result_count=len(results),
                metadata={
                    "memory_type": memory_type,
                    "importance": importance,
                    "levels_created": list(results.keys())
                }
            )
            
            return results
            
        except Exception as e:
            # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è –æ—à–∏–±–æ–∫
            duration_ms = (time.time() - start_time) * 1000
            track_operation_performance(
                operation_type="add_memory",
                operation_name="memory_orchestrator_add",
                user_id=user_id,
                duration_ms=duration_ms,
                success=False,
                error_message=str(e),
                metadata={
                    "memory_type": memory_type,
                    "importance": importance
                }
            )
            logger.error(f"Error adding memory: {e}")
            raise
    
    async def search_memory(self, query: MemoryQuery) -> UnifiedMemoryResult:
        """
        –ü–æ–∏—Å–∫ –ø–æ –≤—Å–µ–º —É—Ä–æ–≤–Ω—è–º –ø–∞–º—è—Ç–∏
        
        Args:
            query: –ó–∞–ø—Ä–æ—Å –∫ —Å–∏—Å—Ç–µ–º–µ –ø–∞–º—è—Ç–∏
            
        Returns:
            –£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ–∏—Å–∫–∞
        """
        try:
            start_time = time.time()
            logger.info(f"SearchMemory: user={query.user_id} q='{query.query}' levels={query.memory_levels or 'all'} limit={query.limit} offset={query.offset}")
            
            # –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞
            cache_key = f"search:{query.user_id}:{hash(query.query)}:{query.limit}:{query.offset}"
            cached_result = await self._get_search_cache(cache_key)
            
            if cached_result and self._is_search_cache_fresh(cached_result, query.user_id):
                logger.info(f"Cache hit for search: {query.query}")
                try:
                    inc("search_cache_hits")
                except Exception as e:
                    logger.warning(f"Failed to update search cache hits metrics: {e}")
                
                # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è –∫—ç—à-—Ö–∏—Ç–∞
                duration_ms = (time.time() - start_time) * 1000
                record_search_metrics(
                    query=query.query,
                    search_type="integrated",
                    user_id=query.user_id,
                    duration_ms=duration_ms,
                    results=[],  # –£–ø—Ä–æ—â–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                    cache_hit=True,
                    memory_levels=cached_result.get("levels_searched", []),
                    filters={"limit": query.limit, "offset": query.offset}
                )
                
                track_operation_performance(
                    operation_type="search",
                    operation_name="memory_orchestrator_search_cached",
                    user_id=query.user_id,
                    duration_ms=duration_ms,
                    success=True,
                    result_count=cached_result["total_items"],
                    cache_hit=True,
                    metadata={
                        "query_length": len(query.query),
                        "cache_hit": True
                    }
                )
                
                # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —É–ø—Ä–æ—â–µ–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∏–∑ –∫—ç—à–∞
                return UnifiedMemoryResult(
                    query=cached_result["query"],
                    user_id=cached_result["user_id"],
                    results={},  # –ü—É—Å—Ç—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è –∫—ç—à–∞
                    total_items=cached_result["total_items"],
                    processing_time=0.001,  # –ë—ã—Å—Ç—Ä—ã–π –æ—Ç–≤–µ—Ç –∏–∑ –∫—ç—à–∞
                    recommendations=cached_result.get("recommendations", [])
                )
            
            results: Dict[str, MemoryResult] = {}
            total_items = 0
            level_times: Dict[str, float] = {}
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Ä–æ–≤–Ω–∏ –¥–ª—è –ø–æ–∏—Å–∫–∞ (–≤–∫–ª—é—á–∞–µ–º Sensor Buffer –∏ FTS5)
            levels_to_search = query.memory_levels or ["sensor", "working", "short_term", "episodic", "semantic", "graph", "procedural", "fts5"]
            
            # –ü–æ–∏—Å–∫ –ø–æ –∫–∞–∂–¥–æ–º—É —É—Ä–æ–≤–Ω—é ‚Äî –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
            async def _search_sensor():
                """–ü–æ–∏—Å–∫ –≤ Sensor Buffer - —Å–∞–º—ã–π –±—ã—Å—Ç—Ä—ã–π –∏ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–π"""
                _t0 = datetime.now()
                try:
                    # –ü–æ–ª—É—á–∞–µ–º –Ω–µ–¥–∞–≤–Ω–∏–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ Sensor Buffer
                    recent_data = self.sensor_buffer.get_recent_data(
                        user_id=query.user_id,
                        seconds=30,  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 30 —Å–µ–∫—É–Ω–¥
                        limit=query.limit
                    )
                    
                    # –ü—Ä–æ—Å—Ç–æ–π —Ç–µ–∫—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫ –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É
                    matching_items = []
                    query_lower = query.query.lower()
                    
                    for data in recent_data:
                        if data.text_content and query_lower in data.text_content.lower():
                            matching_items.append(data)
                    
                    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                    items = matching_items[query.offset:query.offset + query.limit]
                    
                    level_times["sensor"] = (datetime.now() - _t0).total_seconds()
                    try:
                        set_gauge("search_time_sensor_seconds", level_times["sensor"])
                    except Exception as e:
                        logger.warning(f"Failed to update sensor search time metrics: {e}")
                    
                    return ("sensor", MemoryResult(
                        level="sensor",
                        items=items,
                        relevance_scores=[1.0] * len(items),  # –í—ã—Å–æ–∫–∞—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –¥–ª—è —Å–µ–Ω—Å–æ—Ä–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
                        total_found=len(matching_items)
                    ))
                except Exception as e:
                    logger.warning(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –≤ Sensor Buffer: {e}")
                    level_times["sensor"] = (datetime.now() - _t0).total_seconds()
                    return ("sensor", MemoryResult(
                        level="sensor",
                        items=[],
                        relevance_scores=[],
                        total_found=0
                    ))
            
            async def _search_working():
                _t0 = datetime.now()
                full = await self.working_memory.search_context(query.user_id, query.query, query.limit + query.offset)
                items = full[query.offset:query.offset + query.limit]
                level_times["working"] = (datetime.now() - _t0).total_seconds()
                try:
                    set_gauge("search_time_working_seconds", level_times["working"])
                except Exception as e:
                    logger.warning(f"Failed to update working memory search time metrics: {e}")
                now = datetime.now()
                scores = [
                    compute_priority(
                        base_importance=getattr(it, "importance", 0.5),
                        emotional_intensity=None,
                        recency_hours=max(0.0, (now - getattr(it, "timestamp", now)).total_seconds() / 3600.0),
                    ) for it in items
                ]
                return ("working", MemoryResult("working", items, scores, len(items)))

            async def _search_short_term():
                _t0 = datetime.now()
                full = await self.short_term_memory.search_events(query.user_id, query.query, hours=24, limit=query.limit + query.offset)
                items = full[query.offset:query.offset + query.limit]
                level_times["short_term"] = (datetime.now() - _t0).total_seconds()
                try:
                    set_gauge("search_time_short_term_seconds", level_times["short_term"])
                except Exception as e:
                    logger.warning(f"Failed to update short term memory search time metrics: {e}")
                now = datetime.now()
                scores = [
                    compute_priority(
                        base_importance=getattr(it, "importance", 0.5),
                        emotional_intensity=None,
                        recency_hours=max(0.0, (now - getattr(it, "timestamp", now)).total_seconds() / 3600.0),
                    ) for it in items
                ]
                return ("short_term", MemoryResult("short_term", items, scores, len(items)))

            async def _search_episodic():
                _t0 = datetime.now()
                full = await self.episodic_memory.search_experiences(query.user_id, query.query, days=30, limit=query.limit + query.offset)
                items = full[query.offset:query.offset + query.limit]
                level_times["episodic"] = (datetime.now() - _t0).total_seconds()
                try:
                    set_gauge("search_time_episodic_seconds", level_times["episodic"])
                except Exception as e:
                    logger.warning(f"Failed to update episodic memory search time metrics: {e}")
                now = datetime.now()
                scores = [
                    compute_priority(
                        base_importance=getattr(it, "importance", 0.5),
                        emotional_intensity=None,
                        recency_hours=max(0.0, (now - getattr(it, "timestamp", now)).total_seconds() / 3600.0),
                    ) for it in items
                ]
                return ("episodic", MemoryResult("episodic", items, scores, len(items)))

            async def _search_semantic():
                _t0 = datetime.now()
                full = await self.semantic_memory.search_knowledge(query.user_id, query.query, limit=query.limit + query.offset)
                items = full[query.offset:query.offset + query.limit]
                level_times["semantic"] = (datetime.now() - _t0).total_seconds()
                try:
                    set_gauge("search_time_semantic_seconds", level_times["semantic"])
                except Exception as e:
                    logger.warning(f"Failed to update semantic memory search time metrics: {e}")
                now = datetime.now()
                scores = []
                for it in items:
                    recency_h = None
                    try:
                        recency_h = max(0.0, (now - getattr(it, "last_accessed", now)).total_seconds() / 3600.0)
                    except Exception:
                        recency_h = None
                    scores.append(
                        compute_priority(
                            base_importance=getattr(it, "importance", 0.5),
                            last_accessed=getattr(it, "last_accessed", None),
                            access_count=getattr(it, "access_count", 0),
                            emotional_intensity=None,
                            recency_hours=recency_h,
                        )
                    )
                return ("semantic", MemoryResult("semantic", items, scores, len(items)))

            async def _search_graph():
                _t0 = datetime.now()
                full = await self.graph_memory.search_nodes(query.user_id, query.query, limit=query.limit + query.offset)
                items = full[query.offset:query.offset + query.limit]
                level_times["graph"] = (datetime.now() - _t0).total_seconds()
                try:
                    set_gauge("search_time_graph_seconds", level_times["graph"])
                except Exception as e:
                    logger.warning(f"Failed to update graph memory search time metrics: {e}")
                return ("graph", MemoryResult("graph", items, [0.5] * len(items), len(items)))

            async def _search_procedural():
                _t0 = datetime.now()
                full = await self.procedural_memory.search_skills(query.user_id, query.query, limit=query.limit + query.offset)
                items = full[query.offset:query.offset + query.limit]
                level_times["procedural"] = (datetime.now() - _t0).total_seconds()
                try:
                    set_gauge("search_time_procedural_seconds", level_times["procedural"])
                except Exception as e:
                    logger.warning(f"Failed to update procedural memory search time metrics: {e}")
                return ("procedural", MemoryResult("procedural", items, [0.5] * len(items), len(items)))

            async def _search_fts5():
                """–ü–æ–∏—Å–∫ –≤ FTS5 - –±—ã—Å—Ç—Ä—ã–π –∫–ª—é—á–µ–≤–æ–π –ø–æ–∏—Å–∫"""
                _t0 = datetime.now()
                try:
                    from ..search import get_fts5_engine
                    fts5_engine = await get_fts5_engine()
                    
                    # –í—ã–ø–æ–ª–Ω—è–µ–º FTS5 –ø–æ–∏—Å–∫
                    fts5_results = await fts5_engine.search(
                        query=query.query,
                        user_id=query.user_id,
                        limit=query.limit
                    )
                    
                    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Ñ–æ—Ä–º–∞—Ç –¥–ª—è MemoryResult
                    items = []
                    for result in fts5_results:
                        # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Å—Ç–æ–π –æ–±—ä–µ–∫—Ç —Å –Ω—É–∂–Ω—ã–º–∏ –ø–æ–ª—è–º–∏
                        item = type('FTS5Result', (), {
                            'id': result["memory_id"],
                            'content': result["content"],
                            'user_id': result["user_id"],
                            'memory_type': result["memory_type"],
                            'importance': result["importance"],
                            'created_at': datetime.fromtimestamp(result["created_at"]),
                            'metadata': {"fts_score": result["fts_score"], "search_type": "keyword"}
                        })()
                        items.append(item)
                    
                    _t1 = datetime.now()
                    level_times["fts5"] = (_t1 - _t0).total_seconds()
                    logger.debug(f"FTS5 search completed: {len(items)} items in {level_times['fts5']:.3f}s")
                    
                    # –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –≤—Ä–µ–º–µ–Ω–∏ –ø–æ–∏—Å–∫–∞
                    try:
                        set_gauge("memory_search_time_fts5_seconds", level_times["fts5"])
                    except Exception as e:
                        logger.warning(f"Failed to update FTS5 search time metrics: {e}")
                    
                    return ("fts5", MemoryResult("fts5", items, [result["fts_score"] for result in fts5_results], len(items)))
                except Exception as e:
                    logger.warning(f"FTS5 search failed: {e}")
                    return ("fts5", MemoryResult("fts5", [], [], 0))

            tasks = []
            if "sensor" in levels_to_search:
                tasks.append(_search_sensor())
            if "working" in levels_to_search:
                tasks.append(_search_working())
            if "short_term" in levels_to_search:
                tasks.append(_search_short_term())
            if "episodic" in levels_to_search:
                tasks.append(_search_episodic())
            if "semantic" in levels_to_search:
                tasks.append(_search_semantic())
            if "graph" in levels_to_search:
                tasks.append(_search_graph())
            if "procedural" in levels_to_search:
                tasks.append(_search_procedural())
            if "fts5" in levels_to_search:
                tasks.append(_search_fts5())

            if tasks:
                results_list = await asyncio.gather(*tasks, return_exceptions=True)
                for res in results_list:
                    if isinstance(res, Exception):
                        continue
                    if isinstance(res, tuple) and len(res) == 2:
                        level, mem_result = res
                        results[level] = mem_result
                        total_items += mem_result.total_found
            
            processing_time = time.time() - start_time
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
            recommendations = await self._generate_recommendations(query, results)
            logger.info(f"SearchMemory: total_items={total_items} duration={processing_time:.2f}s")
            try:
                record_event("search_memory", {
                    "user_id": query.user_id,
                    "q": query.query,
                    "levels": levels_to_search,
                    "limit": query.limit,
                    "offset": query.offset,
                    "total": total_items,
                    "duration": time.time() - start_time,
                    "level_times": level_times,
                })
            except Exception as e:
                logger.warning(f"Failed to update search result metrics: {e}")
            
            # –°–æ–∑–¥–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            result = UnifiedMemoryResult(
                query=query.query,
                user_id=query.user_id,
                results=results,
                total_items=total_items,
                processing_time=processing_time,
                recommendations=recommendations
            )
            
            # –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ –∫—ç—à (—Ç–æ–ª—å–∫–æ —Å—Ç–∞–±–∏–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã)
            if self._is_search_result_stable(result):
                # –°–æ–∑–¥–∞–µ–º —É–ø—Ä–æ—â–µ–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è
                cache_data = {
                    "query": result.query,
                    "user_id": result.user_id,
                    "total_items": result.total_items,
                    "processing_time": result.processing_time,
                    "recommendations": result.recommendations,
                    "timestamp": time.time(),
                    "results_summary": {
                        level: {
                            "total_found": mem_result.total_found,
                            "items_count": len(mem_result.items) if mem_result.items else 0
                        }
                        for level, mem_result in result.results.items()
                    }
                }
                
                await self._set_search_cache(cache_key, cache_data, ttl_sec=300)  # 5 –º–∏–Ω—É—Ç
                logger.info(f"Search result cached: {query.query}")
                try:
                    inc("search_cache_saves")
                except Exception as e:
                    logger.warning(f"Failed to update search cache saves metrics: {e}")
            
            # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–æ–∏—Å–∫–∞
            duration_ms = (time.time() - start_time) * 1000
            total_results = sum(len(mem_result.items) if mem_result.items else 0 for mem_result in result.results.values())
            avg_relevance = 0.0
            if result.results:
                all_scores = []
                for mem_result in result.results.values():
                    if mem_result.relevance_scores:
                        all_scores.extend(mem_result.relevance_scores)
                avg_relevance = sum(all_scores) / len(all_scores) if all_scores else 0.0
            
            # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ –ø–æ–∏—Å–∫–∞ - –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û!
            try:
                record_search_metrics(
                    query=query.query,
                    search_type="integrated",
                    user_id=query.user_id,
                    duration_ms=duration_ms,
                    results=[],  # –£–ø—Ä–æ—â–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                    cache_hit=False,
                    memory_levels=list(result.results.keys()),
                    filters={"limit": query.limit, "offset": query.offset}
                )
                logger.debug("‚úÖ Search metrics recorded successfully")
            except Exception as e:
                logger.error(f"üö® CRITICAL: Failed to record search metrics: {e}")
                logger.error("üö® Monitoring fallback: using in-memory metrics")
            
            # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –æ–±—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ - –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û!
            try:
                track_operation_performance(
                    operation_type="search",
                    operation_name="memory_orchestrator_search",
                    user_id=query.user_id,
                    duration_ms=duration_ms,
                    success=True,
                    result_count=total_results,
                    accuracy_score=avg_relevance,
                    metadata={
                        "query_length": len(query.query),
                        "levels_searched": list(result.results.keys()),
                        "limit": query.limit,
                        "offset": query.offset
                    }
                )
                logger.debug("‚úÖ Performance metrics tracked successfully")
            except Exception as e:
                logger.error(f"üö® CRITICAL: Failed to track performance metrics: {e}")
                logger.error("üö® Monitoring fallback: using in-memory metrics")
            
            return result
            
        except Exception as e:
            # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è –æ—à–∏–±–æ–∫ –ø–æ–∏—Å–∫–∞
            duration_ms = (time.time() - start_time) * 1000
            track_operation_performance(
                operation_type="search",
                operation_name="memory_orchestrator_search",
                user_id=query.user_id,
                duration_ms=duration_ms,
                success=False,
                error_message=str(e),
                metadata={
                    "query_length": len(query.query),
                    "limit": query.limit,
                    "offset": query.offset
                }
            )
            logger.error(f"Error searching memory: {e}")
            raise

    async def process_message(self, user_id: str, content: str,
                              importance: float = 0.5,
                              context: Optional[str] = None,
                              participants: Optional[List[str]] = None,
                              location: Optional[str] = None,
                              emotion_data: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """–í—ã—Å–æ–∫–æ—É—Ä–æ–≤–Ω–µ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Ö–æ–¥—è—â–µ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è.

        –í—ã–ø–æ–ª–Ω—è–µ—Ç: –∞–≤—Ç–æ–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é —Å–æ–±—ã—Ç–∏—è, –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ñ–∞–∫—Ç–æ–≤ (–ø—Ä–∏ –≤—ã—Å–æ–∫–æ–π –≤–∞–∂–Ω–æ—Å—Ç–∏),
        –∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —É—Ä–æ–≤–Ω—è–º —á–µ—Ä–µ–∑ add_memory.
        """
        try:
            # –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –∞–≤—Ç–æ–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
            try:
                auto_type = await self.event_classifier.classify(content)
            except Exception:
                auto_type = "general"

            results = await self.add_memory(
                content=content,
                user_id=user_id,
                memory_type=auto_type,
                importance=importance,
                emotion_data=emotion_data,
                context=context,
                participants=participants or [],
                location=location
            )
            return {"success": True, "auto_type": auto_type, "results": results}
        except Exception as e:
            logger.error(f"Error processing message: {e}")
            return {"success": False, "error": str(e)}
    
    async def get_memory_context(self, user_id: str, context_type: str = "full") -> Dict[str, Any]:
        """
        –ü–æ–ª—É—á–∏—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø–∞–º—è—Ç–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        
        Args:
            user_id: ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            context_type: –¢–∏–ø –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (full, recent, important)
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –≤—Å–µ—Ö —É—Ä–æ–≤–Ω–µ–π
        """
        try:
            context = {}
            
            if context_type in ["full", "recent"]:
                # –†–∞–±–æ—á–∞—è –ø–∞–º—è—Ç—å
                wm_context = await self.working_memory.get_active_context(user_id)
                context["working"] = [item.content for item in wm_context]
                
                # –ö—Ä–∞—Ç–∫–æ–≤—Ä–µ–º–µ–Ω–Ω–∞—è –ø–∞–º—è—Ç—å
                stm_context = await self.short_term_memory.get_recent_events(user_id, hours=24)
                context["short_term"] = [item.content for item in stm_context]
            
            if context_type in ["full", "important"]:
                # –≠–ø–∏–∑–æ–¥–∏—á–µ—Å–∫–∞—è –ø–∞–º—è—Ç—å
                ep_context = await self.episodic_memory.get_significant_events(user_id, min_importance=0.7)
                context["episodic"] = [item.content for item in ep_context]
                
                # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –ø–∞–º—è—Ç—å
                sm_context = await self.semantic_memory.get_knowledge_by_category(user_id, "general")
                context["semantic"] = [item.content for item in sm_context]
            
            return context
            
        except Exception as e:
            logger.error(f"Error getting memory context: {e}")
            return {}
    
    async def get_memory_stats(self, user_id: str) -> Dict[str, Any]:
        """
        –ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –≤—Å–µ—Ö —É—Ä–æ–≤–Ω–µ–π –ø–∞–º—è—Ç–∏
        
        Args:
            user_id: ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π –≤—Å–µ—Ö —É—Ä–æ–≤–Ω–µ–π
        """
        try:
            stats = {}
            
            # –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫–∞–∂–¥–æ–≥–æ —É—Ä–æ–≤–Ω—è
            stats["working"] = await self.working_memory.get_stats(user_id)
            stats["short_term"] = await self.short_term_memory.get_stats(user_id)
            stats["episodic"] = await self.episodic_memory.get_stats(user_id)
            stats["semantic"] = await self.semantic_memory.get_stats(user_id)
            stats["graph"] = await self.graph_memory.get_stats(user_id)
            stats["procedural"] = await self.procedural_memory.get_stats(user_id)
            
            # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            total_items = sum(
                level_stats.get("total_items", 0) 
                for level_stats in stats.values() 
                if isinstance(level_stats, dict)
            )
            
            stats["total"] = {
                "total_items": total_items,
                "levels_active": len([s for s in stats.values() if isinstance(s, dict) and s.get("total_items", 0) > 0])
            }
            
            return stats
            
        except Exception as e:
            logger.error(f"Error getting memory stats: {e}")
            return {"error": str(e)}
    
    async def cleanup_memories(self, user_id: str):
        """
        –û—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö –≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏–π –Ω–∞ –≤—Å–µ—Ö —É—Ä–æ–≤–Ω—è—Ö
        
        Args:
            user_id: ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        """
        try:
            # –û—á–∏—â–∞–µ–º –∫–∞–∂–¥—ã–π —É—Ä–æ–≤–µ–Ω—å
            await self.working_memory._cleanup_old_memories(user_id)
            await self.short_term_memory._cleanup_old_memories(user_id)
            await self.episodic_memory._cleanup_old_memories(user_id)
            await self.semantic_memory._cleanup_old_memories(user_id)
            if self.graph_memory:
                try:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –º–µ—Ç–æ–¥–∞ —á–µ—Ä–µ–∑ getattr
                    cleanup_method = getattr(self.graph_memory, '_cleanup_old_memories', None)
                    if cleanup_method:
                        await cleanup_method(user_id)
                    else:
                        logger.warning("GraphMemoryManagerSQLite does not support _cleanup_old_memories")
                except AttributeError:
                    logger.warning("GraphMemoryManagerSQLite does not support _cleanup_old_memories")
            await self.procedural_memory._cleanup_old_memories(user_id)
            
            logger.info(f"Cleaned up memories for user {user_id}")
            
        except Exception as e:
            logger.error(f"Error cleaning up memories: {e}")
    
    async def consolidate_memories(self, user_id: str) -> Dict[str, int]:
        """–ö–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏—è –º–µ–∂–¥—É —É—Ä–æ–≤–Ω—è–º–∏ –ø–∞–º—è—Ç–∏ –ø–æ –ø—Ä–∞–≤–∏–ª–∞–º –≤–æ–∑—Ä–∞—Å—Ç–∞.

        –ü—Ä–∞–≤–∏–ª–∞:
        - Working -> Short-term, –µ—Å–ª–∏ —Å—Ç–∞—Ä—à–µ 1 —á–∞—Å–∞
        - Short-term -> Episodic, –µ—Å–ª–∏ —Å—Ç–∞—Ä—à–µ 7 –¥–Ω–µ–π
        - Episodic -> Semantic, –µ—Å–ª–∏ —Å—Ç–∞—Ä—à–µ 30 –¥–Ω–µ–π –∏ importance >= 0.5
        """
        moved_counts = {"working_to_short_term": 0, "short_term_to_episodic": 0, "episodic_to_semantic": 0}

        # Working -> Short-term
        try:
            old_working = await self.working_memory.get_older_than(user_id, timedelta(hours=1), limit=200)
            for item in old_working:
                await self.short_term_memory.add_event(
                    content=item.content,
                    user_id=user_id,
                    importance=item.importance,
                    confidence=min(1.0, max(0.0, item.importance * 0.8 + 0.2)),
                    event_type="conversation",
                    location=None,
                    participants=[],
                    emotion_data=item.emotion_data,
                    related_memories=[]
                )
                await self.working_memory.remove_memory(item.id)
                moved_counts["working_to_short_term"] += 1
        except Exception as e:
            logger.error(f"Error consolidating Working -> Short-term for user {user_id}: {e}")

        # Short-term -> Episodic (older than 7 days)
        try:
            old_short = await self.short_term_memory.get_older_than(user_id, timedelta(days=7), limit=500)
            for item in old_short:
                await self.episodic_memory.add_experience(
                    content=item.content,
                    user_id=user_id,
                    importance=item.importance,
                    confidence=min(1.0, max(0.0, item.importance * 0.8 + 0.2)),
                    event_type=item.event_type or "experience",
                    location=item.location,
                    participants=item.participants or [],
                    emotion_data=item.emotion_data,
                    related_memories=item.related_memories or [],
                    significance=min(1.0, 0.4 + item.importance * 0.6),
                    vividness=0.5,
                    context=None
                )
                await self.short_term_memory.remove_memory(item.id)
                moved_counts["short_term_to_episodic"] += 1
        except Exception as e:
            logger.error(f"Error consolidating Short-term -> Episodic for user {user_id}: {e}")

        # Episodic -> Semantic (older than 30 days and importance >= 0.5)
        try:
            old_epi = await self.episodic_memory.get_older_than(user_id, timedelta(days=30), limit=1000)
            for item in old_epi:
                if item.importance >= 0.5:
                    await self.semantic_memory.add_knowledge(
                        content=item.content,
                        user_id=user_id,
                        knowledge_type="fact",
                        category="experience",
                        confidence=0.6,
                        source="episodic",
                        related_concepts=item.related_memories or [],
                        tags=[item.event_type] if item.event_type else [],
                        importance=item.importance
                    )
                    await self.episodic_memory.remove_memory(item.id)
                    moved_counts["episodic_to_semantic"] += 1
        except Exception as e:
            logger.error(f"Error consolidating Episodic -> Semantic for user {user_id}: {e}")

        logger.info(f"Consolidation finished for user {user_id}: {moved_counts}")
        return moved_counts

    async def get_known_user_ids(self, max_per_level: int = 2000) -> Set[str]:
        """–°–æ–±—Ä–∞—Ç—å —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ user_id, –∑–∞–º–µ—á–µ–Ω–Ω—ã–µ –≤ —É—Ä–æ–≤–Ω—è—Ö –ø–∞–º—è—Ç–∏ (Chroma-—É—Ä–æ–≤–Ω–∏)."""
        user_ids: Set[str] = set()
        try:
            # Working
            try:
                res = self.working_memory.collection.get(limit=max_per_level, include=["metadatas"])
                metadatas = res.get("metadatas", [])
                if isinstance(metadatas, list):
                    for meta in metadatas:
                        uid = meta.get("user_id")
                        if uid and isinstance(uid, str):
                            user_ids.add(uid)
            except Exception as e:
                logger.warning(f"Failed to collect user IDs from working memory: {e}")
            # Short-term
            try:
                res = self.short_term_memory.collection.get(limit=max_per_level, include=["metadatas"])
                metadatas = res.get("metadatas", [])
                if isinstance(metadatas, list):
                    for meta in metadatas:
                        uid = meta.get("user_id")
                        if uid and isinstance(uid, str):
                            user_ids.add(uid)
            except Exception as e:
                logger.warning(f"Failed to collect user IDs from short term memory: {e}")
            # Episodic
            try:
                res = self.episodic_memory.collection.get(limit=max_per_level, include=["metadatas"])
                metadatas = res.get("metadatas", [])
                if isinstance(metadatas, list):
                    for meta in metadatas:
                        uid = meta.get("user_id")
                        if uid and isinstance(uid, str):
                            user_ids.add(uid)
            except Exception as e:
                logger.warning(f"Failed to collect user IDs from episodic memory: {e}")
            # Semantic
            try:
                res = self.semantic_memory.collection.get(limit=max_per_level, include=["metadatas"])
                metadatas = res.get("metadatas", [])
                if isinstance(metadatas, list):
                    for meta in metadatas:
                        uid = meta.get("user_id")
                        if uid and isinstance(uid, str):
                            user_ids.add(uid)
            except Exception as e:
                logger.warning(f"Failed to collect user IDs from semantic memory: {e}")
        except Exception as e:
            logger.error(f"Error collecting known user ids: {e}")
        return user_ids
    
    async def _extract_concepts(self, content: str, user_id: str = "claude_ai", importance: float = 0.5) -> tuple[List[str], dict]:
        """
        –ò–∑–≤–ª–µ—á—å –∫–ª—é—á–µ–≤—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞ —Å –ø–æ–º–æ—â—å—é EntityExtractor
        
        Args:
            content: –¢–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            user_id: ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –¥–ª—è EntityExtractor
            
        Returns:
            –°–ø–∏—Å–æ–∫ –∫–æ–Ω—Ü–µ–ø—Ü–∏–π
        """
        try:
            # –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à –∫–æ–Ω—Ü–µ–ø—Ü–∏–π (—Ç–æ–ª—å–∫–æ –¥–ª—è –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤)
            import hashlib
            content_hash = hashlib.md5(content.encode()).hexdigest()
            cache_key = f"concepts:{content_hash}"
            
            # –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å –∏–∑ –∫—ç—à–∞ (–±—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞)
            try:
                from ..cache.sqlite_cache import get_sqlite_cache
                cache = await get_sqlite_cache()
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º timeout –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫—ç—à–∞
                cached_concepts = await asyncio.wait_for(cache.get(cache_key), timeout=1.0)
                if cached_concepts:
                    concepts = cached_concepts.get('concepts', [])
                    entity_types = cached_concepts.get('entity_types', {})
                    logger.debug(f"Cache hit for concepts: {len(concepts)} concepts, {len(entity_types)} entity_types")
                    
                    # –ï—Å–ª–∏ entity_types –ø—É—Å—Ç–æ–π, –Ω–æ –µ—Å—Ç—å –∫–æ–Ω—Ü–µ–ø—Ç—ã, –ø–æ–ø—Ä–æ–±—É–µ–º –∏–∑–≤–ª–µ—á—å —Ç–∏–ø—ã –∏–∑ LLM
                    if not entity_types and concepts:
                        logger.debug("Entity types empty in cache, extracting from LLM...")
                        unified_result = await self._process_memory_unified(content, user_id, importance)
                        entities = unified_result.get("entities", [])
                        for entity in entities:
                            if isinstance(entity, dict):
                                entity_name = entity.get("text", "")
                                entity_type = entity.get("type", "")
                                if entity_name in concepts:
                                    entity_types[entity_name] = entity_type.lower()
                        logger.debug(f"Extracted entity types from LLM: {entity_types}")
                    
                    return concepts, entity_types
            except (Exception, asyncio.TimeoutError) as e:
                logger.debug(f"Cache miss for concepts: {e}")
            
            # –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –ò—Å–ø–æ–ª—å–∑—É–µ–º –µ–¥–∏–Ω—ã–π LLM –≤—ã–∑–æ–≤ –≤–º–µ—Å—Ç–æ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö
            unified_result = await self._process_memory_unified(content, user_id, importance)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ –∏–∑ –µ–¥–∏–Ω–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Ç–∏–ø–æ–≤
            concepts = []
            entity_types = {}  # –°–ª–æ–≤–∞—Ä—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ç–∏–ø–æ–≤ —Å—É—â–Ω–æ—Å—Ç–µ–π
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Å—É—â–Ω–æ—Å—Ç–∏ –ø–æ —Ç–∏–ø–∞–º –∏–∑ –µ–¥–∏–Ω–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            entities = unified_result.get("entities", [])
            logger.debug(f"Unified processing found {len(entities)} entities: {entities}")
            for entity in entities:
                if isinstance(entity, dict):
                    entity_name = entity.get("text", "")
                    entity_type = entity.get("type", "")
                    entity_importance = entity.get("importance", 0.5)
                    
                    logger.debug(f"Entity: name='{entity_name}', type='{entity_type}', importance={entity_importance}")
                    
                    # –†–∞—Å—à–∏—Ä—è–µ–º —Ñ–∏–ª—å—Ç—Ä –¥–ª—è –≤–∫–ª—é—á–µ–Ω–∏—è –≤—Å–µ—Ö —Ç–∏–ø–æ–≤ –∏–∑ –ø—Ä–æ–º–ø—Ç–∞
                    entity_type_lower = entity_type.lower()
                    if any(t in entity_type_lower for t in ['concept', 'technology', 'organization', 'person', 'service', 'date']):
                        concepts.append(entity_name)
                        entity_types[entity_name] = entity_type_lower  # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–∏–ø —Å—É—â–Ω–æ—Å—Ç–∏
                        logger.debug(f"Added concept: {entity_name} (type: {entity_type_lower})")
                    else:
                        logger.debug(f"Skipped entity type: {entity_type}")
                else:
                    logger.warning(f"Invalid entity format: {entity}")
            
            # –ï—Å–ª–∏ EntityExtractor –Ω–µ –Ω–∞—à–µ–ª —Å—É—â–Ω–æ—Å—Ç–µ–π, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback
            if not concepts:
                logger.warning("EntityExtractor –Ω–µ –Ω–∞—à–µ–ª —Å—É—â–Ω–æ—Å—Ç–µ–π, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback")
                concepts = await self._extract_concepts_fallback(content)
                logger.info(f"Fallback extracted {len(concepts)} concepts: {concepts}")
                
                # –ï—Å–ª–∏ fallback —Ç–æ–∂–µ –Ω–µ –¥–∞–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, —Å–æ–∑–¥–∞–µ–º –±–∞–∑–æ–≤—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞
                if not concepts:
                    logger.warning("Fallback –Ω–µ –¥–∞–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, —Å–æ–∑–¥–∞–µ–º –±–∞–∑–æ–≤—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏")
                    concepts = await self._create_basic_concepts_async(content)
                    logger.info(f"Basic concepts created: {len(concepts)} concepts: {concepts}")
            
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏, –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏
            unique_concepts = list(set(concepts))
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –¥–ª–∏–Ω–µ (–±–æ–ª–µ–µ –¥–ª–∏–Ω–Ω—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ –æ–±—ã—á–Ω–æ –±–æ–ª–µ–µ –∑–Ω–∞—á–∏–º—ã)
            sorted_concepts = sorted(unique_concepts, key=lambda x: len(x), reverse=True)
            final_concepts = sorted_concepts[:10]  # –ú–∞–∫—Å–∏–º—É–º 10 –∫–æ–Ω—Ü–µ–ø—Ü–∏–π
            
            # –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –∫—ç—à (–Ω–µ –±–ª–æ–∫–∏—Ä—É–µ—Ç –æ—Å–Ω–æ–≤–Ω–æ–π –ø–æ—Ç–æ–∫)
            task = asyncio.create_task(self._cache_concepts_async(cache_key, final_concepts, entity_types))
            self._add_background_task(task)
            
            return final_concepts, entity_types
            
        except Exception as e:
            logger.error(f"Error extracting concepts with EntityExtractor: {e}")
            # Fallback –Ω–∞ –ø—Ä–æ—Å—Ç–æ–π –º–µ—Ç–æ–¥
            fallback_concepts = await self._extract_concepts_fallback(content)
            return fallback_concepts, {}
    
    async def _cache_concepts_async(self, cache_key: str, concepts: List[str], entity_types: Optional[dict] = None):
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–π –≤ –∫—ç—à"""
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–ª–∞–≥ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
            if self._shutdown_event.is_set():
                logger.debug("Shutdown requested, skipping concept caching")
                return
                
            from ..cache.sqlite_cache import get_sqlite_cache
            cache = await get_sqlite_cache()
            cache_data = {'concepts': concepts}
            if entity_types and isinstance(entity_types, dict):
                cache_data['entity_types'] = list(entity_types.keys())  # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º dict –≤ List[str]
            await cache.set(cache_key, cache_data, ttl_sec=3600)  # –ö—ç—à –Ω–∞ 1 —á–∞—Å
            logger.debug(f"Cached concepts: {len(concepts)} concepts")
        except Exception as e:
            logger.debug(f"Failed to cache concepts: {e}")
    
    async def _extract_concepts_fallback(self, content: str) -> List[str]:
        """
        Fallback –º–µ—Ç–æ–¥ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–æ–Ω—Ü–µ–ø—Ü–∏–π (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)
        
        Args:
            content: –¢–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            
        Returns:
            –°–ø–∏—Å–æ–∫ –∫–æ–Ω—Ü–µ–ø—Ü–∏–π
        """
        try:
            import re
            
            # –û—á–∏—â–∞–µ–º —Ç–µ–∫—Å—Ç –æ—Ç –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏ –∏ –ø—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
            cleaned_content = re.sub(r'[^\w\s]', ' ', content.lower())
            words = cleaned_content.split()
            
            # –†—É—Å—Å–∫–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ (–æ—Å–Ω–æ–≤–Ω—ã–µ)
            russian_stop_words = {
                "–∏", "–≤", "–Ω–∞", "—Å", "–ø–æ", "–¥–ª—è", "–æ—Ç", "–¥–æ", "–∏–∑", "–∫", "—É", "–æ", "–æ–±", "—á—Ç–æ", "–∫–∞–∫", "–≥–¥–µ", "–∫–æ–≥–¥–∞", "–ø–æ—á–µ–º—É",
                "—ç—Ç–æ", "—Ç–æ", "–æ–Ω", "–æ–Ω–∞", "–æ–Ω–æ", "–æ–Ω–∏", "–º—ã", "–≤—ã", "—è", "—Ç—ã", "–µ–≥–æ", "–µ—ë", "–∏—Ö", "–Ω–∞—à", "–≤–∞—à", "–º–æ–π", "—Ç–≤–æ–π",
                "–±—ã—Ç—å", "–µ—Å—Ç—å", "–±—ã–ª", "–±—ã–ª–∞", "–±—ã–ª–æ", "–±—ã–ª–∏", "–±—É–¥–µ—Ç", "–±—É–¥—É—Ç", "–∏–º–µ—Ç—å", "–∏–º–µ–µ—Ç", "–∏–º–µ–ª", "–∏–º–µ–ª–∞", "–∏–º–µ–ª–∏",
                "–¥–µ–ª–∞—Ç—å", "–¥–µ–ª–∞–µ—Ç", "–¥–µ–ª–∞–ª", "–¥–µ–ª–∞–ª–∞", "–¥–µ–ª–∞–ª–∏", "–º–æ–∂–µ—Ç", "—Ö–æ—Ç–µ—Ç—å", "—Ö–æ—á–µ—Ç", "—Ö–æ—Ç–µ–ª", "—Ö–æ—Ç–µ–ª–∞", "—Ö–æ—Ç–µ–ª–∏", 
                "–Ω—É–∂–Ω–æ", "–Ω—É–∂–µ–Ω", "–Ω—É–∂–Ω–∞", "–Ω—É–∂–Ω—ã", "–º–æ–∂–Ω–æ", "–Ω–µ–ª—å–∑—è", "–æ—á–µ–Ω—å", "–±–æ–ª–µ–µ", "–º–µ–Ω–µ–µ", "—Å–∞–º—ã–π", "—Å–∞–º–∞—è", "—Å–∞–º–æ–µ", 
                "—Å–∞–º—ã–µ", "–≤—Å–µ", "–≤—Å—è", "–≤—Å—ë", "–∫–∞–∂–¥—ã–π", "–∫–∞–∂–¥–∞—è", "–∫–∞–∂–¥–æ–µ", "–∫–∞–∂–¥—ã–µ", "–¥—Ä—É–≥–æ–π", "–¥—Ä—É–≥–∞—è", "–¥—Ä—É–≥–æ–µ", "–¥—Ä—É–≥–∏–µ", 
                "–Ω–æ–≤—ã–π", "–Ω–æ–≤–∞—è", "–Ω–æ–≤–æ–µ", "–Ω–æ–≤—ã–µ", "—Å—Ç–∞—Ä—ã–π", "—Å—Ç–∞—Ä–∞—è", "—Å—Ç–∞—Ä–æ–µ", "—Å—Ç–∞—Ä—ã–µ", "–±–æ–ª—å—à–æ–π", "–±–æ–ª—å—à–∞—è", "–±–æ–ª—å—à–æ–µ", 
                "–±–æ–ª—å—à–∏–µ", "–º–∞–ª–µ–Ω—å–∫–∏–π", "–º–∞–ª–µ–Ω—å–∫–∞—è", "–º–∞–ª–µ–Ω—å–∫–æ–µ", "–º–∞–ª–µ–Ω—å–∫–∏–µ", "—Ö–æ—Ä–æ—à–∏–π", "—Ö–æ—Ä–æ—à–∞—è", "—Ö–æ—Ä–æ—à–µ–µ", "—Ö–æ—Ä–æ—à–∏–µ", 
                "–ø–ª–æ—Ö–æ–π", "–ø–ª–æ—Ö–∞—è", "–ø–ª–æ—Ö–æ–µ", "–ø–ª–æ—Ö–∏–µ", "–ø–µ—Ä–≤—ã–π", "–ø–µ—Ä–≤–∞—è", "–ø–µ—Ä–≤–æ–µ", "–ø–µ—Ä–≤—ã–µ", "–ø–æ—Å–ª–µ–¥–Ω–∏–π", "–ø–æ—Å–ª–µ–¥–Ω—è—è", 
                "–ø–æ—Å–ª–µ–¥–Ω–µ–µ", "–ø–æ—Å–ª–µ–¥–Ω–∏–µ", "–≥–ª–∞–≤–Ω—ã–π", "–≥–ª–∞–≤–Ω–∞—è", "–≥–ª–∞–≤–Ω–æ–µ", "–≥–ª–∞–≤–Ω—ã–µ", "–æ—Å–Ω–æ–≤–Ω–æ–π", "–æ—Å–Ω–æ–≤–Ω–∞—è", "–æ—Å–Ω–æ–≤–Ω–æ–µ", 
                "–æ—Å–Ω–æ–≤–Ω—ã–µ", "–≤–∞–∂–Ω—ã–π", "–≤–∞–∂–Ω–∞—è", "–≤–∞–∂–Ω–æ–µ", "–≤–∞–∂–Ω—ã–µ"
            }
            
            # –ê–Ω–≥–ª–∏–π—Å–∫–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ (–æ—Å–Ω–æ–≤–Ω—ã–µ)
            english_stop_words = {
                "the", "a", "an", "and", "or", "but", "in", "on", "at", "to", "for", "of", "with", "by", "is", "are", "was", 
                "were", "be", "been", "have", "has", "had", "do", "does", "did", "will", "would", "could", "should", "may", 
                "might", "can", "must", "this", "that", "these", "those", "i", "you", "he", "she", "it", "we", "they", "me",
                "him", "her", "us", "them", "my", "your", "his", "her", "its", "our", "their", "mine", "yours", "hers", "ours",
                "theirs", "am", "are", "is", "was", "were", "being", "been", "have", "has", "had", "having", "do", "does",
                "did", "doing", "will", "would", "could", "should", "may", "might", "can", "must", "shall", "ought", "need",
                "dare", "used", "get", "got", "getting", "go", "went", "gone", "going", "come", "came", "coming", "see", "saw",
                "seen", "seeing", "know", "knew", "known", "knowing", "think", "thought", "thinking", "take", "took", "taken",
                "taking", "give", "gave", "given", "giving", "make", "made", "making", "find", "found", "finding", "tell",
                "told", "telling", "ask", "asked", "asking", "work", "worked", "working", "seem", "seemed", "seeming", "feel",
                "felt", "feeling", "try", "tried", "trying", "leave", "left", "leaving", "call", "called", "calling", "move",
                "moved", "moving", "play", "played", "playing", "turn", "turned", "turning", "start", "started", "starting",
                "stop", "stopped", "stopping", "show", "showed", "shown", "showing", "hear", "heard", "hearing", "let",
                "letting", "put", "putting", "bring", "brought", "bringing", "begin", "began", "begun", "beginning", "keep",
                "kept", "keeping", "hold", "held", "holding", "write", "wrote", "written", "writing", "provide", "provided",
                "providing", "sit", "sat", "sitting", "stand", "stood", "standing", "lose", "lost", "losing", "pay", "paid",
                "paying", "meet", "met", "meeting", "include", "included", "including", "continue", "continued", "continuing",
                "set", "setting", "learn", "learned", "learning", "change", "changed", "changing", "lead", "led", "leading",
                "understand", "understood", "understanding", "watch", "watched", "watching", "follow", "followed", "following",
                "stop", "stopped", "stopping", "create", "created", "creating", "speak", "spoke", "spoken", "speaking",
                "read", "reading", "allow", "allowed", "allowing", "add", "added", "adding", "spend", "spent", "spending",
                "grow", "grew", "grown", "growing", "open", "opened", "opening", "walk", "walked", "walking", "win", "won",
                "winning", "offer", "offered", "offering", "remember", "remembered", "remembering", "love", "loved", "loving",
                "consider", "considered", "considering", "appear", "appeared", "appearing", "buy", "bought", "buying", "wait",
                "waited", "waiting", "serve", "served", "serving", "die", "died", "dying", "send", "sent", "sending", "expect",
                "expected", "expecting", "build", "built", "building", "stay", "stayed", "staying", "fall", "fell", "fallen",
                "falling", "cut", "cutting", "reach", "reached", "reaching", "kill", "killed", "killing", "remain", "remained",
                "remaining", "suggest", "suggested", "suggesting", "raise", "raised", "raising", "pass", "passed", "passing",
                "sell", "sold", "selling", "require", "required", "requiring", "report", "reported", "reporting", "decide",
                "decided", "deciding", "pull", "pulled", "pulling", "break", "broke", "broken", "breaking", "produce",
                "produced", "producing", "eat", "ate", "eaten", "eating", "teach", "taught", "teaching", "cover", "covered",
                "covering", "catch", "caught", "catching", "draw", "drew", "drawn", "drawing", "choose", "chose", "chosen",
                "choosing", "wear", "wore", "worn", "wearing", "throw", "threw", "thrown", "throwing", "drive", "drove", "driven",
                "driving", "ride", "rode", "ridden", "riding", "fly", "flew", "flown", "flying", "swim", "swam", "swum", "swimming",
                "run", "ran", "running", "jump", "jumped", "jumping", "climb", "climbed", "climbing", "crawl", "crawled", "crawling",
                "slide", "slid", "sliding", "roll", "rolled", "rolling", "spin", "spun", "spinning", "shake", "shook", "shaken",
                "shaking", "bend", "bent", "bending", "stretch", "stretched", "stretching", "squeeze", "squeezed", "squeezing",
                "press", "pressed", "pressing", "push", "pushed", "pushing", "pull", "pulled", "pulling", "lift", "lifted", "lifting",
                "drop", "dropped", "dropping", "throw", "threw", "thrown", "throwing", "catch", "caught", "catching", "grab",
                "grabbed", "grabbing", "hold", "held", "holding", "carry", "carried", "carrying", "move", "moved", "moving",
                "place", "placed", "placing", "put", "putting", "set", "setting", "lay", "laid", "laying", "stand", "stood",
                "standing", "sit", "sat", "sitting", "lie", "lay", "lain", "lying", "sleep", "slept", "sleeping", "wake", "woke",
                "woken", "waking", "rest", "rested", "resting", "relax", "relaxed", "relaxing", "work", "worked", "working",
                "play", "played", "playing", "study", "studied", "studying", "learn", "learned", "learning", "teach", "taught",
                "teaching", "help", "helped", "helping", "support", "supported", "supporting", "care", "cared", "caring", "love",
                "loved", "loving", "like", "liked", "liking", "enjoy", "enjoyed", "enjoying", "hate", "hated", "hating", "dislike",
                "disliked", "disliking", "prefer", "preferred", "preferring", "choose", "chose", "chosen", "choosing", "select",
                "selected", "selecting", "pick", "picked", "picking", "decide", "decided", "deciding", "determine", "determined",
                "determining", "resolve", "resolved", "resolving", "solve", "solved", "solving", "fix", "fixed", "fixing", "repair",
                "repaired", "repairing", "mend", "mended", "mending", "improve", "improved", "improving", "enhance", "enhanced",
                "enhancing", "upgrade", "upgraded", "upgrading", "update", "updated", "updating", "modify", "modified", "modifying",
                "change", "changed", "changing", "alter", "altered", "altering", "adjust", "adjusted", "adjusting", "adapt",
                "adapted", "adapting", "transform", "transformed", "transforming", "convert", "converted", "converting", "turn",
                "turned", "turning", "switch", "switched", "switching", "shift", "shifted", "shifting", "transfer", "transferred",
                "transferring", "move", "moved", "moving", "relocate", "relocated", "relocating", "migrate", "migrated", "migrating",
                "travel", "traveled", "traveling", "journey", "journeyed", "journeying", "trip", "tripped", "tripping", "visit",
                "visited", "visiting", "tour", "toured", "touring", "explore", "explored", "exploring", "discover", "discovered",
                "discovering", "find", "found", "finding", "locate", "located", "locating", "search", "searched", "searching",
                "seek", "sought", "seeking", "look", "looked", "looking", "watch", "watched", "watching", "observe", "observed",
                "observing", "notice", "noticed", "noticing", "see", "saw", "seen", "seeing", "view", "viewed", "viewing", "examine",
                "examined", "examining", "inspect", "inspected", "inspecting", "check", "checked", "checking", "test", "tested",
                "testing", "try", "tried", "trying", "attempt", "attempted", "attempting", "experiment", "experimented", "experimenting",
                "practice", "practiced", "practicing", "exercise", "exercised", "exercising", "train", "trained", "training", "coach",
                "coached", "coaching", "instruct", "instructed", "instructing", "guide", "guided", "guiding", "lead", "led", "leading",
                "direct", "directed", "directing", "manage", "managed", "managing", "control", "controlled", "controlling", "operate",
                "operated", "operating", "run", "ran", "running", "execute", "executed", "executing", "perform", "performed", "performing",
                "accomplish", "accomplished", "accomplishing", "achieve", "achieved", "achieving", "complete", "completed", "completing",
                "finish", "finished", "finishing", "end", "ended", "ending", "stop", "stopped", "stopping", "pause", "paused", "pausing",
                "wait", "waited", "waiting", "delay", "delayed", "delaying", "postpone", "postponed", "postponing", "cancel", "cancelled",
                "cancelling", "abort", "aborted", "aborting", "terminate", "terminated", "terminating", "close", "closed", "closing",
                "shut", "shut", "shutting", "lock", "locked", "locking", "unlock", "unlocked", "unlocking", "open", "opened", "opening",
                "start", "started", "starting", "begin", "began", "begun", "beginning", "launch", "launched", "launching", "initiate",
                "initiated", "initiating", "establish", "established", "establishing", "create", "created", "creating", "make", "made",
                "making", "build", "built", "building", "construct", "constructed", "constructing", "develop", "developed", "developing",
                "design", "designed", "designing", "plan", "planned", "planning", "organize", "organized", "organizing", "arrange",
                "arranged", "arranging", "prepare", "prepared", "preparing", "setup", "setted", "setting", "install", "installed",
                "installing", "configure", "configured", "configuring", "customize", "customized", "customizing", "personalize",
                "personalized", "personalizing", "tailor", "tailored", "tailoring", "adapt", "adapted", "adapting", "adjust", "adjusted",
                "adjusting", "tune", "tuned", "tuning", "calibrate", "calibrated", "calibrating", "optimize", "optimized", "optimizing",
                "maximize", "maximized", "maximizing", "minimize", "minimized", "minimizing", "reduce", "reduced", "reducing", "decrease",
                "decreased", "decreasing", "increase", "increased", "increasing", "expand", "expanded", "expanding", "extend", "extended",
                "extending", "stretch", "stretched", "stretching", "grow", "grew", "grown", "growing", "shrink", "shrank", "shrunk",
                "shrinking", "contract", "contracted", "contracting", "compress", "compressed", "compressing", "squeeze", "squeezed",
                "squeezing", "press", "pressed", "pressing", "push", "pushed", "pushing", "pull", "pulled", "pulling", "drag", "dragged",
                "dragging", "draw", "drew", "drawn", "drawing", "paint", "painted", "painting", "color", "colored", "coloring", "dye",
                "dyed", "dying", "tint", "tinted", "tinting", "shade", "shaded", "shading", "highlight", "highlighted", "highlighting",
                "emphasize", "emphasized", "emphasizing", "stress", "stressed", "stressing", "accent", "accented", "accenting", "focus",
                "focused", "focusing", "concentrate", "concentrated", "concentrating", "center", "centered", "centering", "target",
                "targeted", "targeting", "aim", "aimed", "aiming", "point", "pointed", "pointing", "direct", "directed", "directing",
                "guide", "guided", "guiding", "steer", "steered", "steering", "navigate", "navigated", "navigating", "pilot", "piloted",
                "piloting", "drive", "drove", "driven", "driving", "ride", "rode", "ridden", "riding", "fly", "flew", "flown", "flying",
                "sail", "sailed", "sailing", "cruise", "cruised", "cruising", "float", "floated", "floating", "drift", "drifted", "drifting",
                "flow", "flowed", "flowing", "stream", "streamed", "streaming", "pour", "poured", "pouring", "spill", "spilled", "spilling",
                "leak", "leaked", "leaking", "drip", "dripped", "dripping", "drop", "dropped", "dropping", "fall", "fell", "fallen",
                "falling", "sink", "sank", "sunk", "sinking", "dive", "dove", "dived", "diving", "plunge", "plunged", "plunging", "jump",
                "jumped", "jumping", "leap", "leapt", "leaping", "hop", "hopped", "hopping", "skip", "skipped", "skipping", "bounce",
                "bounced", "bouncing", "spring", "sprang", "sprung", "springing", "bound", "bounded", "bounding", "vault", "vaulted",
                "vaulting", "hurdle", "hurdled", "hurdling", "climb", "climbed", "climbing", "scale", "scaled", "scaling", "ascend",
                "ascended", "ascending", "rise", "rose", "risen", "rising", "lift", "lifted", "lifting", "raise", "raised", "raising",
                "elevate", "elevated", "elevating", "boost", "boosted", "boosting", "promote", "promoted", "promoting", "advance",
                "advanced", "advancing", "progress", "progressed", "progressing", "proceed", "proceeded", "proceeding", "continue",
                "continued", "continuing", "persist", "persisted", "persisting", "endure", "endured", "enduring", "last", "lasted",
                "lasting", "remain", "remained", "remaining", "stay", "stayed", "staying", "keep", "kept", "keeping", "retain", "retained",
                "retaining", "hold", "held", "holding", "grasp", "grasped", "grasping", "grip", "gripped", "gripping", "clutch", "clutched",
                "clutching", "clasp", "clasped", "clasping", "hug", "hugged", "hugging", "embrace", "embraced", "embracing", "cuddle",
                "cuddled", "cuddling", "snuggle", "snuggled", "snuggling", "nestle", "nestled", "nestling", "lean", "leaned", "leaning",
                "rest", "rested", "resting", "recline", "reclined", "reclining", "lie", "lay", "lain", "lying", "sit", "sat", "sitting",
                "squat", "squatted", "squatting", "crouch", "crouched", "crouching", "kneel", "knelt", "kneeling", "bend", "bent", "bending",
                "stoop", "stooped", "stooping", "bow", "bowed", "bowing", "nod", "nodded", "nodding", "shake", "shook", "shaken", "shaking",
                "tremble", "trembled", "trembling", "quiver", "quivered", "quivering", "shiver", "shivered", "shivering", "vibrate",
                "vibrated", "vibrating", "oscillate", "oscillated", "oscillating", "swing", "swung", "swinging", "sway", "swayed", "swaying",
                "rock", "rocked", "rocking", "roll", "rolled", "rolling", "spin", "spun", "spinning", "rotate", "rotated", "rotating",
                "turn", "turned", "turning", "twist", "twisted", "twisting", "wind", "wound", "winding", "coil", "coiled", "coiling",
                "curl", "curled", "curling", "loop", "looped", "looping", "circle", "circled", "circling", "orbit", "orbited", "orbiting",
                "revolve", "revolved", "revolving", "rotate", "rotated", "rotating", "spin", "spun", "spinning", "whirl", "whirled",
                "whirling", "twirl", "twirled", "twirling", "pirouette", "pirouetted", "pirouetting", "dance", "danced", "dancing",
                "waltz", "waltzed", "waltzing", "tango", "tangoed", "tangoing", "foxtrot", "foxtrotted", "foxtrotting", "samba", "sambaed",
                "sambaing", "rumba", "rumbaed", "rumbaing", "cha", "chaed", "chaing", "mambo", "mamboed", "mamboing", "salsa", "salsaed",
                "salsaing", "merengue", "merengued", "merenguing", "bachata", "bachataed", "bachataing", "kizomba", "kizombaed", "kizombaing",
                "zouk", "zouked", "zouking", "reggaeton", "reggaetoned", "reggaetoning", "hip", "hiped", "hiping", "hop", "hopped", "hopping",
                "break", "breaked", "breaking", "pop", "popped", "popping", "lock", "locked", "locking", "popping", "popped", "popping",
                "krump", "krumped", "krumping", "turf", "turfed", "turfing", "flex", "flexed", "flexing", "jook", "jooked", "jooking",
                "buck", "bucked", "bucking", "juke", "juked", "juking", "wop", "wopped", "wopping", "dougie", "dougied", "dougieing",
                "nay", "nayed", "naying", "cat", "catted", "catting", "fish", "fished", "fishing", "stanky", "stankied", "stankying",
                "leg", "legged", "legging", "milly", "millied", "millying", "rock", "rocked", "rocking", "shmoney", "shmoneyed", "shmoneying",
                "yeet", "yeeted", "yeeting", "dab", "dabbed", "dabbing", "floss", "flossed", "flossing", "orange", "oranged", "oranging",
                "justice", "justiced", "justicing", "fortnite", "fortnited", "fortniting", "default", "defaulted", "defaulting", "dance",
                "danced", "dancing", "move", "moved", "moving", "step", "stepped", "stepping", "walk", "walked", "walking", "run", "ran",
                "running", "jog", "jogged", "jogging", "sprint", "sprinted", "sprinting", "dash", "dashed", "dashing", "rush", "rushed",
                "rushing", "hurry", "hurried", "hurrying", "speed", "sped", "speeding", "race", "raced", "racing", "compete", "competed",
                "competing", "contest", "contested", "contesting", "challenge", "challenged", "challenging", "oppose", "opposed", "opposing",
                "resist", "resisted", "resisting", "fight", "fought", "fighting", "battle", "battled", "battling", "war", "warred", "warring",
                "conflict", "conflicted", "conflicting", "struggle", "struggled", "struggling", "strive", "strove", "striven", "striving",
                "endeavor", "endeavored", "endeavoring", "attempt", "attempted", "attempting", "try", "tried", "trying", "test", "tested",
                "testing", "experiment", "experimented", "experimenting", "trial", "trialed", "trialing", "sample", "sampled", "sampling",
                "taste", "tasted", "tasting", "try", "tried", "trying", "experience", "experienced", "experiencing", "feel", "felt", "feeling",
                "sense", "sensed", "sensing", "perceive", "perceived", "perceiving", "notice", "noticed", "noticing", "observe", "observed",
                "observing", "watch", "watched", "watching", "see", "saw", "seen", "seeing", "look", "looked", "looking", "view", "viewed",
                "viewing", "gaze", "gazed", "gazing", "stare", "stared", "staring", "glance", "glanced", "glancing", "peek", "peeked", "peeking",
                "peep", "peeped", "peeping", "peer", "peered", "peering", "scan", "scanned", "scanning", "skim", "skimmed", "skimming",
                "browse", "browsed", "browsing", "search", "searched", "searching", "seek", "sought", "seeking", "hunt", "hunted", "hunting",
                "track", "tracked", "tracking", "trace", "traced", "tracing", "follow", "followed", "following", "pursue", "pursued", "pursuing",
                "chase", "chased", "chasing", "hunt", "hunted", "hunting", "stalk", "stalked", "stalking", "shadow", "shadowed", "shadowing",
                "tail", "tailed", "tailing", "trail", "trailed", "trailing", "pursue", "pursued", "pursuing", "chase", "chased", "chasing",
                "hunt", "hunted", "hunting", "stalk", "stalked", "stalking", "shadow", "shadowed", "shadowing", "tail", "tailed", "tailing",
                "trail", "trailed", "trailing", "pursue", "pursued", "pursuing", "chase", "chased", "chasing", "hunt", "hunted", "hunting",
                "stalk", "stalked", "stalking", "shadow", "shadowed", "shadowing", "tail", "tailed", "tailing", "trail", "trailed", "trailing"
            }
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
            all_stop_words = russian_stop_words | english_stop_words
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏
            concepts = []
            for word in words:
                # –§–∏–ª—å—Ç—Ä—É–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞, —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ –∏ —á–∏—Å–ª–∞
                if (len(word) > 3 and 
                    word not in all_stop_words and 
                    not word.isdigit() and
                    word.isalpha()):
                    concepts.append(word)
            
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏, –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ —á–∞—Å—Ç–æ—Ç–µ
            concept_counts = {}
            for concept in concepts:
                concept_counts[concept] = concept_counts.get(concept, 0) + 1
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —á–∞—Å—Ç–æ—Ç–µ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ø-10
            sorted_concepts = sorted(concept_counts.items(), key=lambda x: x[1], reverse=True)
            return [concept for concept, count in sorted_concepts[:10]]
            
        except Exception as e:
            logger.error(f"Error in fallback concept extraction: {e}")
            return []
    
    async def _create_basic_concepts_async(self, content: str) -> List[str]:
        """
        –°–æ–∑–¥–∞—Ç—å –±–∞–∑–æ–≤—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –≥—Ä–∞—Ñ–∞
        
        Args:
            content: –¢–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            
        Returns:
            –°–ø–∏—Å–æ–∫ –±–∞–∑–æ–≤—ã—Ö –∫–æ–Ω—Ü–µ–ø—Ü–∏–π
        """
        try:
            import re
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–ª–æ–≤–∞ —Å –∑–∞–≥–ª–∞–≤–Ω–æ–π –±—É–∫–≤—ã (–∏–º–µ–Ω–∞, –Ω–∞–∑–≤–∞–Ω–∏—è)
            capitalized_words = re.findall(r'\b[–ê-–Ø–Å][–∞-—è—ë]+\b|\b[A-Z][a-z]+\b', content)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–ª–æ–≤–∞ –≤ –∫–∞–≤—ã—á–∫–∞—Ö
            quoted_words = re.findall(r'"([^"]+)"', content)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã (—Å–ª–æ–≤–∞ —Å —Ü–∏—Ñ—Ä–∞–º–∏ –∏–ª–∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–º–∏ —Å–∏–º–≤–æ–ª–∞–º–∏)
            technical_terms = re.findall(r'\b\w*[0-9]+\w*\b|\b\w*[_-]\w*\b', content)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–ª–∏–Ω–Ω—ã–µ —Å–ª–æ–≤–∞ (–±–æ–ª–µ–µ 6 —Å–∏–º–≤–æ–ª–æ–≤)
            long_words = re.findall(r'\b\w{6,}\b', content)
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏
            concepts = []
            concepts.extend(capitalized_words)
            concepts.extend(quoted_words)
            concepts.extend(technical_terms)
            concepts.extend(long_words)
            
            # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
            unique_concepts = list(set(concepts))
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
            stop_words = {
                "—Ç–µ—Å—Ç", "test", "–º–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤–æ–π", "–ø–∞–º—è—Ç–∏", "—Å–∏—Å—Ç–µ–º—ã", "—Å–∏—Å—Ç–µ–º–∞", "system", "memory", "level", "—É—Ä–æ–≤–µ–Ω—å",
                "–¥–∞–Ω–Ω—ã–µ", "data", "–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è", "information", "–∫–æ–Ω—Ç–µ–Ω—Ç", "content", "—Ç–µ–∫—Å—Ç", "text", "—Å–æ–æ–±—â–µ–Ω–∏–µ", "message"
            }
            
            filtered_concepts = [c for c in unique_concepts if c.lower() not in stop_words]
            
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –º–∞–∫—Å–∏–º—É–º 5 –∫–æ–Ω—Ü–µ–ø—Ü–∏–π
            return filtered_concepts[:5]
            
        except Exception as e:
            logger.error(f"Error creating basic concepts: {e}")
            return []
    
    async def _extract_facts_async(self, content: str, user_id: str, importance: float, max_retries: int = 3):
        """
        –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è —ç–∫—Å—Ç—Ä–∞–∫—Ü–∏—è —Ñ–∞–∫—Ç–æ–≤ —Å retry –º–µ—Ö–∞–Ω–∏–∑–º–æ–º
        
        Args:
            content: –°–æ–¥–µ—Ä–∂–∏–º–æ–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            user_id: ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            importance: –í–∞–∂–Ω–æ—Å—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            max_retries: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫
        """
        for attempt in range(max_retries):
            try:
                logger.info(f"Starting fact extraction (attempt {attempt + 1}/{max_retries}): user={user_id}")
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ñ–∞–∫—Ç—ã
                facts = await self.fact_extractor.extract_facts(content)
                
                if not facts:
                    logger.info(f"No facts extracted for user {user_id}")
                    return
                
                # –î–æ–±–∞–≤–ª—è–µ–º —Ñ–∞–∫—Ç—ã –≤ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é –ø–∞–º—è—Ç—å
                semantic_ids: List[str] = []
                for fact in facts:
                    sm_id = await self.semantic_memory.add_knowledge(
                        content=fact,
                        user_id=user_id,
                        knowledge_type="fact",
                        category="extracted",
                        confidence=validate_confidence(min(1.0, max(0.0, importance * 0.8 + 0.2))),
                        importance=importance
                    )
                    semantic_ids.append(sm_id)
                
                # –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
                try:
                    inc("memory_add_level_semantic")
                    inc("background_tasks_completed")
                except Exception as _e:
                    logger.warning(f"metrics inc failed (semantic-extracted): {_e}")
                
                logger.info(f"Fact extraction completed successfully: user={user_id} facts={len(semantic_ids)}")
                return
                
            except Exception as e:
                logger.error(f"Fact extraction attempt {attempt + 1} failed: {e}")
                
                if attempt == max_retries - 1:
                    # –ü–æ—Å–ª–µ–¥–Ω—è—è –ø–æ–ø—ã—Ç–∫–∞ –Ω–µ—É–¥–∞—á–Ω–∞ - –ª–æ–≥–∏—Ä—É–µ–º –æ—à–∏–±–∫—É
                    logger.error(f"Fact extraction failed after {max_retries} attempts for user {user_id}: {e}")
                    try:
                        inc("background_tasks_failed")
                    except Exception as e:
                        logger.warning(f"Failed to update background tasks failed metrics: {e}")
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º –≤ –æ—á–µ—Ä–µ–¥—å –¥–ª—è –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
                    try:
                        await self._queue_for_retry("fact_extraction", {
                            "user_id": user_id,
                            "content": content,
                            "importance": importance,
                            "retry_count": 0,
                            "max_retries": 3
                        })
                        logger.info(f"Fact extraction queued for retry: user={user_id}")
                    except Exception as retry_error:
                        logger.warning(f"Failed to queue fact extraction for retry: {retry_error}")
                else:
                    # –ñ–¥–µ–º –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–µ–π –ø–æ–ø—ã—Ç–∫–æ–π (exponential backoff)
                    wait_time = 2 ** attempt
                    logger.info(f"Waiting {wait_time} seconds before retry...")
                    await asyncio.sleep(wait_time)

    async def _analyze_relationships_async(self, content: str, user_id: str, added_node_ids: List[str], importance: float, max_retries: int = 3):
        """
        –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –æ—Ç–Ω–æ—à–µ–Ω–∏–π —Å retry –º–µ—Ö–∞–Ω–∏–∑–º–æ–º
        
        Args:
            content: –°–æ–¥–µ—Ä–∂–∏–º–æ–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            user_id: ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            added_node_ids: –°–ø–∏—Å–æ–∫ —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö —É–∑–ª–æ–≤
            importance: –í–∞–∂–Ω–æ—Å—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            max_retries: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫
        """
        for attempt in range(max_retries):
            try:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–ª–∞–≥ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
                if self._shutdown_event.is_set():
                    logger.info(f"Shutdown requested, cancelling relationship analysis for user={user_id}")
                    return
                
                logger.info(f"Starting relationship analysis (attempt {attempt + 1}/{max_retries}): user={user_id} nodes={len(added_node_ids)}")
                
                if not added_node_ids or len(added_node_ids) < 2:
                    logger.info(f"Not enough nodes for relationship analysis: user={user_id}")
                    # –ó–∞–≤–µ—Ä—à–∞–µ–º –∑–∞–¥–∞—á—É –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ
                    try:
                        inc("relationship_analysis_completed")
                        logger.info(f"Relationship analysis completed successfully: user={user_id} relationships=0")
                    except Exception as e:
                        logger.warning(f"Failed to update relationship analysis metrics: {e}")
                    return
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ –¥–ª—è —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è
                concepts, entity_types = await self._extract_concepts(content, user_id)
                concept_to_node = {concept.lower(): nid for concept, nid in zip(concepts, added_node_ids)}
                
                # Advanced: —Å–æ–∑–¥–∞—ë–º —Ç–∏–ø–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä—ë–±—Ä–∞ –º–µ–∂–¥—É –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã–º–∏ –∫–æ–Ω—Ü–µ–ø—Ü–∏—è–º–∏
                adv = advanced_relationships(content)
                relationships_added = 0
                
                logger.info(f"Advanced relationships found: {len(adv)}")
                for src_c, dst_c, rel_type, weight in adv[:10]:
                    # –ò—â–µ–º —É–∑–ª—ã –ø–æ –∏–º–µ–Ω–∏ (–±–µ–∑ —É—á–µ—Ç–∞ —Ä–µ–≥–∏—Å—Ç—Ä–∞)
                    src_id = None
                    dst_id = None
                    for node_id in added_node_ids:
                        # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± —É–∑–ª–µ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
                        # –ü–æ–∫–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç—É—é –ª–æ–≥–∏–∫—É - –∏—â–µ–º –ø–æ –∫–æ–Ω—Ü–µ–ø—Ç–∞–º
                        for concept, nid in zip(concepts, added_node_ids):
                            if nid == node_id and concept.lower() == src_c.lower():
                                src_id = node_id
                            if nid == node_id and concept.lower() == dst_c.lower():
                                dst_id = node_id
                    
                    if src_id and dst_id and src_id != dst_id:
                        await self.graph_memory.add_edge(
                            source_id=src_id,
                            target_id=dst_id,
                            user_id=user_id,
                            relationship_type=rel_type,
                            strength=float(min(1.0, max(0.1, weight)))
                        )
                        relationships_added += 1
                        logger.info(f"Added relationship: {src_c} -> {dst_c} ({rel_type})")
                
                # Fallback: –µ—Å–ª–∏ advanced –Ω–µ –Ω–∞—à–µ–ª —Å–≤—è–∑–µ–π, —Å–æ–∑–¥–∞–µ–º –±–∞–∑–æ–≤—ã–µ —Å–≤—è–∑–∏ –º–µ–∂–¥—É –≤—Å–µ–º–∏ —É–∑–ª–∞–º–∏
                if relationships_added == 0 and len(added_node_ids) >= 2:
                    logger.info("No advanced relationships found, creating basic connections")
                    for i in range(len(added_node_ids)):
                        for j in range(i + 1, len(added_node_ids)):
                            try:
                                await self.graph_memory.add_edge(
                                    source_id=added_node_ids[i],
                                    target_id=added_node_ids[j],
                                    user_id=user_id,
                                    relationship_type="related",
                                    strength=0.5
                                )
                                relationships_added += 1
                                logger.info(f"Added basic relationship: {added_node_ids[i]} -> {added_node_ids[j]}")
                            except Exception as e:
                                logger.warning(f"Failed to add basic relationship: {e}")
                
                # Naive fallback: –µ—Å–ª–∏ —É–∑–ª–æ–≤ >=2 –∏ –Ω–µ—Ç —è–≤–Ω—ã—Ö —Å–≤—è–∑–µ–π ‚Äî –¥–æ–±–∞–≤–∏–º similar_to –æ–¥–Ω–æ–π —Å–≤—è–∑—å—é
                if relationships_added == 0 and len(added_node_ids) >= 2:
                    sentences = [s.strip() for s in content.split('.') if s.strip()]
                    rels = naive_relationships(sentences)
                    if rels:
                        await self.graph_memory.add_edge(
                            source_id=added_node_ids[0],
                            target_id=added_node_ids[1],
                            user_id=user_id,
                            relationship_type=rels[0][2],
                            strength=min(1.0, 0.5 + importance / 2)
                        )
                        relationships_added += 1
                
                # –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
                try:
                    inc("background_tasks_completed")
                except Exception as _e:
                    logger.warning(f"metrics inc failed (relationship-analysis): {_e}")
                
                logger.info(f"Relationship analysis completed successfully: user={user_id} relationships={relationships_added}")
                return
                
            except Exception as e:
                logger.error(f"Relationship analysis attempt {attempt + 1} failed: {e}")
                
                if attempt == max_retries - 1:
                    # –ü–æ—Å–ª–µ–¥–Ω—è—è –ø–æ–ø—ã—Ç–∫–∞ –Ω–µ—É–¥–∞—á–Ω–∞ - –ª–æ–≥–∏—Ä—É–µ–º –æ—à–∏–±–∫—É
                    logger.error(f"Relationship analysis failed after {max_retries} attempts for user {user_id}: {e}")
                    try:
                        inc("background_tasks_failed")
                    except Exception as e:
                        logger.warning(f"Failed to update background tasks failed metrics: {e}")
                else:
                    # –ñ–¥–µ–º –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–µ–π –ø–æ–ø—ã—Ç–∫–æ–π (exponential backoff)
                    wait_time = 2 ** attempt
                    logger.info(f"Waiting {wait_time} seconds before retry...")
                    await asyncio.sleep(wait_time)

    async def _get_search_cache(self, cache_key: str) -> Optional[dict]:
        """–ü–æ–ª—É—á–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ–∏—Å–∫–∞ –∏–∑ –∫—ç—à–∞"""
        try:
            try:
                from ..cache.memory_cache import cache_get
            except ImportError:
                from cache.memory_cache import cache_get
            return await cache_get(cache_key)
        except Exception as e:
            logger.warning(f"Failed to get search cache: {e}")
            return None

    async def _set_search_cache(self, cache_key: str, result: dict, ttl_sec: int = 300):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ–∏—Å–∫–∞ –≤ –∫—ç—à"""
        try:
            try:
                from ..cache.memory_cache import cache_set
            except ImportError:
                from cache.memory_cache import cache_set
            await cache_set(cache_key, result, ttl_sec)
        except Exception as e:
            logger.warning(f"Failed to set search cache: {e}")

    def _is_search_cache_fresh(self, cached_result: dict, user_id: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Å–≤–µ–∂–µ—Å—Ç—å –∫—ç—à–∞ –ø–æ–∏—Å–∫–∞"""
        try:
            cache_timestamp = cached_result.get("timestamp", 0)
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –æ–±–Ω–æ–≤–ª—è–ª–∞—Å—å –ª–∏ –ø–∞–º—è—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            last_memory_update = self._get_last_memory_update(user_id)
            return cache_timestamp >= last_memory_update
        except Exception as e:
            logger.warning(f"Failed to check cache freshness: {e}")
            return False

    def _is_search_result_stable(self, result: UnifiedMemoryResult) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –ø–æ–∏—Å–∫–∞ –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è"""
        try:
            # –ö—ç—à–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏ –æ–Ω–∏ –Ω–µ —Å–ª–∏—à–∫–æ–º —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã
            if result.total_items == 0:
                return False
            
            # –ù–µ –∫—ç—à–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å –æ—á–µ–Ω—å –º–∞–ª–µ–Ω—å–∫–∏–º –ª–∏–º–∏—Ç–æ–º (–º–æ–≥—É—Ç –±—ã—Ç—å —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–º–∏)
            if hasattr(result, 'query') and len(result.query) < 3:
                return False
            
            # –ö—ç—à–∏—Ä—É–µ–º —Å—Ç–∞–±–∏–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            return True
        except Exception as e:
            logger.warning(f"Failed to check result stability: {e}")
            return False

    def _get_last_memory_update(self, user_id: str) -> float:
        """–ü–æ–ª—É—á–∏—Ç—å –≤—Ä–µ–º—è –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –ø–∞–º—è—Ç–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
        try:
            # –ü—Ä–æ—Å—Ç–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è - –º–æ–∂–Ω–æ —É–ª—É—á—à–∏—Ç—å —Å –ø–æ–º–æ—â—å—é –º–µ—Ç—Ä–∏–∫
            import time
            return time.time() - 3600  # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ –ø–∞–º—è—Ç—å –æ–±–Ω–æ–≤–ª—è–ª–∞—Å—å –≤ –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞—Å
        except Exception:
            return 0.0

    async def _invalidate_user_search_cache(self, user_id: str):
        """–ò–Ω–≤–∞–ª–∏–¥–∏—Ä–æ–≤–∞—Ç—å –∫—ç—à –ø–æ–∏—Å–∫–∞ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
        logger.info(f"Starting cache invalidation for user {user_id}")
        try:
            try:
                from ..cache.memory_cache import cache_delete_prefix
                logger.info(f"Imported cache_delete_prefix from ..cache.memory_cache")
            except ImportError:
                from cache.memory_cache import cache_delete_prefix
                logger.info(f"Imported cache_delete_prefix from cache.memory_cache")
            
            logger.info(f"Calling cache_delete_prefix for user {user_id}")
            deleted_count = await cache_delete_prefix(f"search:{user_id}:")
            logger.info(f"cache_delete_prefix returned {deleted_count} for user {user_id}")
            
            if deleted_count > 0:
                logger.info(f"Invalidated {deleted_count} search cache entries for user {user_id}")
                try:
                    inc("search_cache_invalidations")
                except Exception as e:
                    logger.warning(f"Failed to update search cache invalidations metrics: {e}")
        except Exception as e:
            logger.error(f"Failed to invalidate search cache for user {user_id}: {e}")
            import traceback
            logger.error(f"Cache invalidation traceback: {traceback.format_exc()}")

    async def consolidate_memory(self, user_id: str, level: Optional[str] = None) -> Dict[str, Any]:
        """
        –ó–∞–ø—É—Å—Ç–∏—Ç—å –∫–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏—é –ø–∞–º—è—Ç–∏
        
        Args:
            user_id: ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –¥–ª—è –∫–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏–∏
            level: –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π —É—Ä–æ–≤–µ–Ω—å –¥–ª—è –∫–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏–∏
        """
        try:
            logger.info(f"Starting memory consolidation for level: {level or 'all'}")
            
            if level:
                # –ö–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —É—Ä–æ–≤–Ω—è
                result = await self.consolidator.consolidate_memory_level(level, user_id)
                return {
                    "success": True,
                    "level": level,
                    "result": result
                }
            else:
                # –ö–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏—è –≤—Å–µ—Ö —É—Ä–æ–≤–Ω–µ–π
                result = await self.consolidator.consolidate_all_memory(user_id)
                return result
                
        except Exception as e:
            logger.error(f"Error during memory consolidation: {e}")
            return {
                "success": False,
                "error": str(e)
            }
    
    async def get_consolidation_stats(self) -> Dict[str, Any]:
        """
        –ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏–∏ –ø–∞–º—è—Ç–∏
        """
        try:
            return await self.consolidator.get_consolidation_stats()
        except Exception as e:
            logger.error(f"Error getting consolidation stats: {e}")
            return {"error": str(e)}
    
    async def schedule_consolidation(self, user_id: str, interval_hours: int = 24):
        """
        –ó–∞–ø–ª–∞–Ω–∏—Ä–æ–≤–∞—Ç—å –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫—É—é –∫–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏—é –ø–∞–º—è—Ç–∏
        
        Args:
            user_id: ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –¥–ª—è –∫–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏–∏
            interval_hours: –ò–Ω—Ç–µ—Ä–≤–∞–ª –º–µ–∂–¥—É –∫–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏—è–º–∏ –≤ —á–∞—Å–∞—Ö
        """
        try:
            # –î–æ–±–∞–≤–ª—è–µ–º —Ñ–ª–∞–≥ –¥–ª—è graceful shutdown
            self._consolidation_running = True
            while self._consolidation_running:
                await asyncio.sleep(interval_hours * 3600)  # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Å–µ–∫—É–Ω–¥—ã
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–ª–∞–≥ –ø–µ—Ä–µ–¥ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ–º
                if not self._consolidation_running:
                    break
                    
                logger.info(f"Starting scheduled memory consolidation for user {user_id}...")
                result = await self.consolidate_memory(user_id)
                
                if result.get("success"):
                    logger.info(f"Scheduled consolidation completed: {result.get('total_removed', 0)} items removed")
                else:
                    logger.error(f"Scheduled consolidation failed: {result.get('error', 'Unknown error')}")
                    
        except asyncio.CancelledError:
            logger.info("Scheduled consolidation cancelled")
        except Exception as e:
            logger.error(f"Error in scheduled consolidation: {e}")
        finally:
            self._consolidation_running = False
    
    def stop_scheduled_consolidation(self):
        """–û—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –∫–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏–∏"""
        self._consolidation_running = False
        logger.info("Scheduled consolidation stop requested")
    
    async def cleanup_old_memories(self, days: int = 365) -> Dict[str, Any]:
        """
        –û—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö –≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏–π
        
        Args:
            days: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–Ω–µ–π –¥–ª—è —É–¥–µ—Ä–∂–∞–Ω–∏—è –≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏–π
            
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—á–∏—Å—Ç–∫–∏
        """
        try:
            logger.info(f"Starting cleanup of memories older than {days} days")
            result = await self.consolidator.cleanup_old_memories(str(days))
            return result
        except Exception as e:
            logger.error(f"Error during memory cleanup: {e}")
            return {
                "success": False,
                "error": str(e)
            }

    async def _generate_recommendations(self, query: MemoryQuery, results: Dict[str, MemoryResult]) -> List[str]:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞
        
        Args:
            query: –ò—Å—Ö–æ–¥–Ω—ã–π –∑–∞–ø—Ä–æ—Å
            results: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
        """
        try:
            recommendations = []
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
            if results.get("working") and results["working"].total_found > 0:
                recommendations.append("–£ –≤–∞—Å –µ—Å—Ç—å –∞–∫—Ç–∏–≤–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø–æ —ç—Ç–æ–π —Ç–µ–º–µ")
            
            if results.get("episodic") and results["episodic"].total_found > 0:
                recommendations.append("–ù–∞–π–¥–µ–Ω—ã –∑–Ω–∞—á–∏–º—ã–µ –≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏—è, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å –∑–∞–ø—Ä–æ—Å–æ–º")
            
            if results.get("semantic") and results["semantic"].total_found > 0:
                recommendations.append("–î–æ—Å—Ç—É–ø–Ω—ã –∑–Ω–∞–Ω–∏—è –ø–æ —ç—Ç–æ–π —Ç–µ–º–µ")
            
            if results.get("procedural") and results["procedural"].total_found > 0:
                recommendations.append("–ï—Å—Ç—å –Ω–∞–≤—ã–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –ø–æ–º–æ—á—å")
            
            # –ï—Å–ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –º–∞–ª–æ, –ø—Ä–µ–¥–ª–∞–≥–∞–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—ã
            if sum(r.total_found for r in results.values()) < 3:
                recommendations.append("–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –∑–∞–ø—Ä–æ—Å –∏–ª–∏ –¥–æ–±–∞–≤–∏—Ç—å –±–æ–ª—å—à–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞")
            
            return recommendations
            
        except Exception as e:
            logger.error(f"Error generating recommendations: {e}")
            return []
    
    async def _create_summary_memory(self, user_id: str, recent_memories: List[Dict[str, Any]]) -> Optional[str]:
        """
        –°–æ–∑–¥–∞–µ—Ç —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—é –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏–π
        
        Args:
            user_id: ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            recent_memories: –°–ø–∏—Å–æ–∫ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏–π –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏
            
        Returns:
            ID —Å–æ–∑–¥–∞–Ω–Ω–æ–π —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ
        """
        try:
            if not recent_memories:
                logger.warning(f"No memories to summarize for user {user_id}")
                return None
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏–π
            combined_content = "\n".join([
                f"[{mem.get('type', 'unknown')}] {mem.get('content', '')}" 
                for mem in recent_memories
            ])
            
            # –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –ò—Å–ø–æ–ª—å–∑—É–µ–º –µ–¥–∏–Ω—ã–π LLM –≤—ã–∑–æ–≤ –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏
            if self.llm_provider:
                unified_result = await self.llm_provider.process_memory_unified(
                    content=combined_content,
                    tasks=["summary"]
                )
            else:
                unified_result = {"summary": combined_content[:200] + "..."}
            summary_content = unified_result.get("summary", combined_content[:200] + "...")
            
            # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏
            summary_metadata = {
                "type": "summary",
                "source_count": len(recent_memories),
                "source_types": list(set(mem.get('type', 'unknown') for mem in recent_memories)),
                "created_at": datetime.now().isoformat(),
                "ttl_days": 180  # TTL 180 –¥–Ω–µ–π –∫–∞–∫ —É–∫–∞–∑–∞–Ω–æ –≤ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è—Ö
            }
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—é –≤ semantic memory (–¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–µ —Ö—Ä–∞–Ω–µ–Ω–∏–µ)
            summary_id = await self.semantic_memory.add_knowledge(
                content=summary_content,
                user_id=user_id,
                importance=0.8,  # –í—ã—Å–æ–∫–∞—è –≤–∞–∂–Ω–æ—Å—Ç—å –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏
                knowledge_type="summary",
                source="memory_consolidation"
            )
            
            logger.info(f"Created summary memory: {summary_id} for user {user_id} with {len(recent_memories)} source memories")
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
            try:
                inc("summary_memories_created")
                set_gauge("summary_memory_source_count", len(recent_memories))
            except Exception as e:
                logger.warning(f"Failed to update summary metrics: {e}")
            
            return summary_id
            
        except Exception as e:
            logger.error(f"Error creating summary memory for user {user_id}: {e}")
            return None
    
    async def _background_post_processing(self, user_id: str, content: str, results: Dict[str, Any], memory_type: str, importance: float) -> None:
        """
        –§–æ–Ω–æ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø–æ—Å–ª–µ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –ø–∞–º—è—Ç–∏ (–Ω–µ –±–ª–æ–∫–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç)
        """
        logger.info(f"Background post-processing started for user {user_id}, results: {results}")
        try:
            # –ò–Ω–≤–∞–ª–∏–¥–∏—Ä—É–µ–º –∫—ç—à –ø–æ–∏—Å–∫–∞ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            logger.info(f"About to invalidate search cache for user {user_id}")
            await self._invalidate_user_search_cache(user_id)
            logger.info(f"Search cache invalidated for user {user_id}")
            
            # FTS5 –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –¥–ª—è –∫–ª—é—á–µ–≤–æ–≥–æ –ø–æ–∏—Å–∫–∞
            logger.info(f"About to start FTS5 indexing for user {user_id}")
            try:
                logger.info(f"Starting FTS5 indexing for user {user_id}")
                from ..search import get_fts5_engine
                fts5_engine = await get_fts5_engine()
                logger.info(f"FTS5 engine obtained for user {user_id}")
                
                # –°–æ–∑–¥–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π ID –¥–ª—è FTS5 (–∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–≤—ã–π –¥–æ—Å—Ç—É–ø–Ω—ã–π ID)
                fts5_id = None
                for level, level_id in results.items():
                    if isinstance(level_id, list) and level_id:
                        fts5_id = level_id[0]
                        break
                    elif isinstance(level_id, str):
                        fts5_id = level_id
                        break
                
                logger.info(f"FTS5 ID found: {fts5_id} for user {user_id}")
                
                if fts5_id:
                    await fts5_engine.index_memory(
                        memory_id=fts5_id,
                        content=content,
                        user_id=user_id,
                        memory_type=memory_type,
                        importance=importance
                    )
                    logger.info(f"Memory {fts5_id} indexed in FTS5 successfully")
            except Exception as e:
                logger.error(f"Failed to index memory in FTS5: {e}")
                import traceback
                logger.error(f"FTS5 indexing traceback: {traceback.format_exc()}")
                
        except Exception as e:
            logger.error(f"Background post-processing failed for user {user_id}: {e}")

    async def _check_and_create_summary_async(self, user_id: str) -> None:
        """
        –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∏ —Å–æ–∑–¥–∞–Ω–∏—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ (–Ω–µ –±–ª–æ–∫–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç)
        
        Args:
            user_id: ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        """
        try:
            summary_id = await self._check_and_create_summary(user_id)
            if summary_id:
                logger.info(f"Summary created asynchronously for user {user_id}: {summary_id}")
        except Exception as e:
            logger.error(f"Failed to create summary asynchronously for user {user_id}: {e}")
        finally:
            # –û—á–∏—â–∞–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—É—é –∑–∞–¥–∞—á—É –∏ lock
            try:
                if user_id in self._summary_task_locks:
                    async with self._summary_task_locks[user_id]:
                        if user_id in self._summary_tasks:
                            del self._summary_tasks[user_id]
                        # –û—á–∏—â–∞–µ–º lock –µ—Å–ª–∏ –æ–Ω –±–æ–ª—å—à–µ –Ω–µ –Ω—É–∂–µ–Ω
                        del self._summary_task_locks[user_id]
            except Exception as cleanup_error:
                logger.warning(f"Failed to cleanup summary task for user {user_id}: {cleanup_error}")

    async def _check_and_create_summary(self, user_id: str) -> Optional[str]:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å —Å–æ–∑–¥–∞–Ω–∏—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ –∏ —Å–æ–∑–¥–∞–µ—Ç –µ—ë –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
        
        Args:
            user_id: ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            
        Returns:
            ID —Å–æ–∑–¥–∞–Ω–Ω–æ–π —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ –∏–ª–∏ None
        """
        try:
            # –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –ê—Ç–æ–º–∞—Ä–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—á–µ—Ç—á–∏–∫–∞ –ø–æ–¥ –æ–¥–Ω–∏–º lock
            async with self._summary_counters_lock:
                if user_id not in self._summary_counters:
                    self._summary_counters[user_id] = 0
                self._summary_counters[user_id] += 1
                final_count = self._summary_counters[user_id]
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–Ω–æ –ª–∏ —Å–æ–∑–¥–∞–≤–∞—Ç—å —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—é
            if final_count < self._summary_threshold:
                return None
            
            logger.info(f"Creating summary for user {user_id} (threshold reached: {final_count})")
            
            # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ –≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏—è –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏
            recent_memories = []
            
            # –°–æ–±–∏—Ä–∞–µ–º –∏–∑ –≤—Å–µ—Ö —É—Ä–æ–≤–Ω–µ–π –ø–∞–º—è—Ç–∏
            for level_name, manager in [
                ("working", self.working_memory),
                ("short_term", self.short_term_memory), 
                ("episodic", self.episodic_memory)
            ]:
                try:
                    memories = await manager.get_memories(user_id, limit=20)
                    for mem in memories:
                        recent_memories.append({
                            "content": getattr(mem, 'content', str(mem)),
                            "type": level_name,
                            "timestamp": getattr(mem, 'created_at', datetime.now())
                        })
                except Exception as e:
                    logger.warning(f"Failed to get memories from {level_name} for summary: {e}")
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏ (–Ω–æ–≤—ã–µ —Å–Ω–∞—á–∞–ª–∞)
            recent_memories.sort(key=lambda x: x.get('timestamp', datetime.min), reverse=True)
            
            # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 20 –≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏–π –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏
            recent_memories = recent_memories[:20]
            
            # –°–æ–∑–¥–∞–µ–º —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—é
            summary_id = await self._create_summary_memory(user_id, recent_memories)
            
            # Thread-safe —Å–±—Ä–æ—Å —Å—á–µ—Ç—á–∏–∫–∞ –ø–æ—Å–ª–µ —Å–æ–∑–¥–∞–Ω–∏—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏
            if summary_id:
                async with self._summary_counters_lock:
                    self._summary_counters[user_id] = 0
                logger.info(f"Summary created successfully for user {user_id}, counter reset")
            
            return summary_id
            
        except Exception as e:
            logger.error(f"Error in summary check for user {user_id}: {e}")
            return None
    
    async def _process_memory_unified(self, content: str, user_id: str, importance: float) -> Dict[str, Any]:
        """
        –ï–¥–∏–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø–∞–º—è—Ç–∏ —á–µ—Ä–µ–∑ –æ–¥–∏–Ω LLM –≤—ã–∑–æ–≤
        –ó–∞–º–µ–Ω—è–µ—Ç –æ—Ç–¥–µ–ª—å–Ω—ã–µ –≤—ã–∑–æ–≤—ã extract_entities, extract_facts, summarize_content
        
        Args:
            content: –°–æ–¥–µ—Ä–∂–∏–º–æ–µ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
            user_id: ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            importance: –í–∞–∂–Ω–æ—Å—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏: entities, facts, summary
        """
        try:
            logger.info(f"Starting unified memory processing: user={user_id}, importance={importance}")
            # –£–õ–£–ß–®–ï–ù–ù–ê–Ø –û–ë–†–ê–ë–û–¢–ö–ê –ö–û–†–û–¢–ö–ò–• –°–û–û–ë–©–ï–ù–ò–ô
            content_length = len(content.strip())
            if content_length < 3:
                # –û—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è (–º–µ–Ω–µ–µ 3 —Å–∏–º–≤–æ–ª–æ–≤) - —Ç–æ–ª—å–∫–æ –±–∞–∑–æ–≤–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
                logger.info(f"Very short content: {content_length} chars - basic processing")
                return {
                    "entities": [],
                    "facts": [],
                    "summary": content.strip()
                }
            elif content_length < 10:
                # –ö–æ—Ä–æ—Ç–∫–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è (3-9 —Å–∏–º–≤–æ–ª–æ–≤) - —É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
                logger.info(f"Short content: {content_length} chars - simplified processing")
                # –î–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –¥–µ–ª–∞–µ–º –±–∞–∑–æ–≤–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π
                simplified_entities = []
                simplified_facts = []
                
                # –ü—Ä–æ—Å—Ç–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π –∏–∑ –∫–æ—Ä–æ—Ç–∫–∏—Ö —Å–æ–æ–±—â–µ–Ω–∏–π
                words = content.strip().split()
                for word in words:
                    if len(word) > 2:  # –°–ª–æ–≤–∞ –¥–ª–∏–Ω–Ω–µ–µ 2 —Å–∏–º–≤–æ–ª–æ–≤
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å–ª–æ–≤–æ –∏–º–µ–Ω–µ–º —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–º –∏–ª–∏ –≤–∞–∂–Ω—ã–º —Ç–µ—Ä–º–∏–Ω–æ–º
                        if word[0].isupper() or word.lower() in ['–¥–∞', '–Ω–µ—Ç', '–æ–∫', '—Ö–æ—Ä–æ—à–æ', '–ø–ª–æ—Ö–æ', '–¥–∞–≤–∞–π', '—Å—Ç–æ–ø', '–Ω–∞—á–∞—Ç—å', '–∫–æ–Ω–µ—Ü']:
                            simplified_entities.append({
                                "text": word,
                                "type": "keyword",
                                "confidence": 0.8
                            })
                
                # –ü—Ä–æ—Å—Ç—ã–µ —Ñ–∞–∫—Ç—ã –¥–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö —Å–æ–æ–±—â–µ–Ω–∏–π
                if content.strip().lower() in ['–¥–∞', 'yes', '–æ–∫', '—Ö–æ—Ä–æ—à–æ', 'good']:
                    simplified_facts.append(["user", "agreed", "to something"])
                elif content.strip().lower() in ['–Ω–µ—Ç', 'no', '–ø–ª–æ—Ö–æ', 'bad']:
                    simplified_facts.append(["user", "disagreed", "with something"])
                elif content.strip().lower() in ['—Å—Ç–æ–ø', 'stop', '–∫–æ–Ω–µ—Ü', 'end']:
                    simplified_facts.append(["user", "requested", "to stop"])
                elif content.strip().lower() in ['–¥–∞–≤–∞–π', 'let\'s', '–Ω–∞—á–∞—Ç—å', 'start']:
                    simplified_facts.append(["user", "requested", "to start"])
                
                return {
                    "entities": simplified_entities,
                    "facts": simplified_facts,
                    "summary": content.strip()
                }

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ —Å–∏—Å—Ç–µ–º–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã (–Ω–æ –≤—Å–µ —Ä–∞–≤–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º)
            if content.strip().startswith('/'):
                logger.info(f"System command detected, processing anyway")
                # –î–ª—è –∫–æ–º–∞–Ω–¥ –¥–æ–±–∞–≤–ª—è–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ñ–∞–∫—Ç–æ–≤
                content = f"SYSTEM COMMAND: {content}\nExtract any useful information or knowledge from this command for memory storage."
            
            logger.info(f"Starting unified memory processing: user={user_id}, importance={importance:.2f}")
            
            # –ï–¥–∏–Ω—ã–π –≤—ã–∑–æ–≤ –∫ LLM –¥–ª—è –≤—Å–µ—Ö –∑–∞–¥–∞—á
            if self.llm_provider:
                unified_result = await self.llm_provider.process_memory_unified(
                    content=content,
                    tasks=["entities", "facts", "summary"]
                )
            else:
                unified_result = {"entities": [], "facts": [], "summary": content[:200] + "..."}
            
            logger.info(f"Unified processing completed: entities={len(unified_result.get('entities', []))}, facts={len(unified_result.get('facts', []))}")
            
            return unified_result
            
        except Exception as e:
            logger.error(f"Error in unified memory processing: {e}")
            # Fallback: –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É
            return {
                "entities": [],
                "facts": [],
                "summary": content[:200] + "..." if len(content) > 200 else content
            }
    
    # ==================== –ü–†–û–ê–ö–¢–ò–í–ù–´–ï –¶–ï–õ–ò ====================
    
    async def start_proactive_timer(self):
        """–ó–∞–ø—É—Å–∫ –ø—Ä–æ–∞–∫—Ç–∏–≤–Ω–æ–≥–æ —Ç–∞–π–º–µ—Ä–∞ —Ü–µ–ª–µ–π"""
        if hasattr(self, 'proactive_timer'):
            await self.proactive_timer.start()
            logger.info("Proactive goals timer started")
    
    async def stop_proactive_timer(self):
        """–û—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø—Ä–æ–∞–∫—Ç–∏–≤–Ω–æ–≥–æ —Ç–∞–π–º–µ—Ä–∞ —Ü–µ–ª–µ–π"""
        if hasattr(self, 'proactive_timer'):
            await self.proactive_timer.stop()
            logger.info("Proactive goals timer stopped")
    
    def create_proactive_goal(self, user_id: str, name: str, description: str, 
                            trigger_type: GoalTriggerType, trigger_value: str,
                            action_type: GoalActionType, action_params: str = "{}") -> str:
        """–°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ–∞–∫—Ç–∏–≤–Ω–æ–π —Ü–µ–ª–∏"""
        goal_id = str(uuid.uuid4())
        
        goal = ProactiveGoal(
            id=goal_id,
            user_id=user_id,
            name=name,
            description=description,
            trigger_type=trigger_type,
            trigger_value=trigger_value,
            action_type=action_type,
            action_params=action_params
        )
        
        if hasattr(self, 'proactive_timer'):
            self.proactive_timer.add_goal(goal)
            logger.info(f"Created proactive goal: {name} ({goal_id})")
        
        return goal_id
    
    def get_proactive_goals(self, user_id: Optional[str] = None) -> List[ProactiveGoal]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–æ–∞–∫—Ç–∏–≤–Ω—ã—Ö —Ü–µ–ª–µ–π"""
        if not hasattr(self, 'proactive_timer'):
            return []
        
        if user_id:
            return self.proactive_timer.get_user_goals(user_id)
        else:
            return self.proactive_timer.get_goals()
    
    def remove_proactive_goal(self, goal_id: str) -> bool:
        """–£–¥–∞–ª–µ–Ω–∏–µ –ø—Ä–æ–∞–∫—Ç–∏–≤–Ω–æ–π —Ü–µ–ª–∏"""
        if hasattr(self, 'proactive_timer'):
            self.proactive_timer.remove_goal(goal_id)
            logger.info(f"Removed proactive goal: {goal_id}")
            return True
        return False
    
    def get_proactive_timer_status(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ –ø—Ä–æ–∞–∫—Ç–∏–≤–Ω–æ–≥–æ —Ç–∞–π–º–µ—Ä–∞"""
        if hasattr(self, 'proactive_timer'):
            return self.proactive_timer.get_status()
        return {"is_running": False, "total_goals": 0, "active_goals": 0}
    
    def _get_sensor_buffer_stats(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ SensorBuffer –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞"""
        try:
            if hasattr(self, 'sensor_buffer') and hasattr(self.sensor_buffer, 'get_stats'):
                try:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –º–µ—Ç–æ–¥–∞ —á–µ—Ä–µ–∑ getattr
                    get_stats_method = getattr(self.sensor_buffer, 'get_stats', None)
                    if get_stats_method:
                        stats = get_stats_method()
                    else:
                        return {"error": "SensorBuffer.get_stats not available"}
                except AttributeError:
                    return {"error": "SensorBuffer.get_stats not available"}
                return {
                    "total_items": stats.get("total_items", 0),
                    "audio_items": stats.get("audio_items", 0),
                    "text_items": stats.get("text_items", 0),
                    "emotion_items": stats.get("emotion_items", 0),
                    "total_added": stats.get("total_added", 0),
                    "total_evicted": stats.get("total_evicted", 0)
                }
            return {"error": "SensorBuffer not available"}
        except Exception as e:
            return {"error": str(e)}
    
    async def _cleanup_sensor_buffer(self) -> float:
        """–û—á–∏—Å—Ç–∫–∞ SensorBuffer"""
        try:
            if hasattr(self, 'sensor_buffer'):
                # –û—á–∏—â–∞–µ–º —Å—Ç–∞—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ (—Å—Ç–∞—Ä—à–µ 30 –º–∏–Ω—É—Ç)
                cutoff_time = datetime.now() - timedelta(minutes=30)  # 30 –º–∏–Ω—É—Ç –Ω–∞–∑–∞–¥
                cleared_count = 0
                
                # –û—á–∏—â–∞–µ–º –∫–∞–∂–¥—ã–π —Ç–∏–ø –±—É—Ñ–µ—Ä–∞
                for buffer_name in ['_audio_buffer', '_video_buffer', '_text_buffer', 
                                  '_emotion_buffer', '_gesture_buffer', '_environment_buffer']:
                    buffer = getattr(self.sensor_buffer, buffer_name, None)
                    if buffer:
                        # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
                        old_items = [item for item in buffer if item.timestamp < cutoff_time]
                        for item in old_items:
                            buffer.remove(item)
                            cleared_count += 1
                
                # –ü—Ä–∏–º–µ—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–Ω–æ–π –ø–∞–º—è—Ç–∏ (1KB –Ω–∞ —ç–ª–µ–º–µ–Ω—Ç)
                freed_mb = cleared_count * 0.001
                
                if cleared_count > 0:
                    logger.info(f"SensorBuffer –æ—á–∏—â–µ–Ω: —É–¥–∞–ª–µ–Ω–æ {cleared_count} —ç–ª–µ–º–µ–Ω—Ç–æ–≤, "
                               f"–æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–æ ~{freed_mb:.2f}MB")
                
                return freed_mb
            return 0.0
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ SensorBuffer: {e}")
            return 0.0
    
    def _get_orchestrator_stats(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ MemoryOrchestrator –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞"""
        try:
            return {
                "background_tasks": len(self._background_tasks),
                "summary_counters": len(self._summary_counters),
                "memory_levels": {
                    "sensor_buffer": hasattr(self, 'sensor_buffer'),
                    "working_memory": hasattr(self, 'working_memory'),
                    "short_term_memory": hasattr(self, 'short_term_memory'),
                    "episodic_memory": hasattr(self, 'episodic_memory'),
                    "semantic_memory": hasattr(self, 'semantic_memory'),
                    "graph_memory": hasattr(self, 'graph_memory'),
                    "procedural_memory": hasattr(self, 'procedural_memory'),
                    "summary_memory": hasattr(self, 'summary_memory')
                },
                "consolidator": hasattr(self, 'consolidator'),
                "proactive_timer": hasattr(self, 'proactive_timer'),
                "memory_monitor": hasattr(self, 'memory_monitor')
            }
        except Exception as e:
            return {"error": str(e)}
    
    async def _cleanup_orchestrator(self) -> float:
        """–û—á–∏—Å—Ç–∫–∞ MemoryOrchestrator"""
        try:
            freed_mb = 0.0
            
            # –û—á–∏—â–∞–µ–º —Å—Ç–∞—Ä—ã–µ —Å—á–µ—Ç—á–∏–∫–∏ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ (—Å—Ç–∞—Ä—à–µ 1 —á–∞—Å–∞)
            if hasattr(self, '_summary_counters'):
                current_time = time.time()
                old_counters = []
                
                for user_id, counter_data in self._summary_counters.items():
                    if isinstance(counter_data, dict) and 'last_update' in counter_data:
                        if current_time - counter_data['last_update'] > 3600:  # 1 —á–∞—Å
                            old_counters.append(user_id)
                
                for user_id in old_counters:
                    del self._summary_counters[user_id]
                
                if old_counters:
                    freed_mb += len(old_counters) * 0.001  # –ü—Ä–∏–º–µ—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
                    logger.info(f"–û—á–∏—â–µ–Ω—ã —Å—Ç–∞—Ä—ã–µ —Å—á–µ—Ç—á–∏–∫–∏ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏: {len(old_counters)} –∑–∞–ø–∏—Å–µ–π")
            
            # –û—á–∏—â–∞–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã–µ —Ñ–æ–Ω–æ–≤—ã–µ –∑–∞–¥–∞—á–∏
            if hasattr(self, '_background_tasks'):
                completed_tasks = [task for task in self._background_tasks if task.done()]
                for task in completed_tasks:
                    self._background_tasks.discard(task)
                
                if completed_tasks:
                    freed_mb += len(completed_tasks) * 0.001  # –ü—Ä–∏–º–µ—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
                    logger.info(f"–û—á–∏—â–µ–Ω—ã –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã–µ —Ñ–æ–Ω–æ–≤—ã–µ –∑–∞–¥–∞—á–∏: {len(completed_tasks)} –∑–∞–¥–∞—á")
            
            return freed_mb
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ MemoryOrchestrator: {e}")
            return 0.0
    
    async def start_memory_monitoring(self):
        """–ó–∞–ø—É—Å–∫ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –ø–∞–º—è—Ç–∏"""
        if hasattr(self, 'memory_monitor') and self.memory_monitor:
            await self.memory_monitor.start_monitoring()
            logger.info("Memory monitoring started")
    
    async def stop_memory_monitoring(self):
        """–û—Å—Ç–∞–Ω–æ–≤–∫–∞ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –ø–∞–º—è—Ç–∏"""
        if hasattr(self, 'memory_monitor') and self.memory_monitor:
            await self.memory_monitor.stop_monitoring()
            logger.info("Memory monitoring stopped")
    
    def get_memory_monitoring_status(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –ø–∞–º—è—Ç–∏"""
        if hasattr(self, 'memory_monitor') and self.memory_monitor:
            return self.memory_monitor.get_monitoring_status()
        return {"error": "Memory monitoring not available"}
    
    async def force_memory_cleanup(self) -> Dict[str, Any]:
        """–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏"""
        if hasattr(self, 'memory_monitor') and self.memory_monitor:
            return await self.memory_monitor.force_cleanup()
        return {"error": "Memory monitoring not available"}
    
    async def start_auto_recovery(self):
        """–ó–∞–ø—É—Å–∫ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è - –û–¢–ö–õ–Æ–ß–ï–ù–û"""
        logger.info("Auto recovery –û–¢–ö–õ–Æ–ß–ï–ù - –∑–∞–ø—É—Å–∫ –Ω–µ–≤–æ–∑–º–æ–∂–µ–Ω")
        return {"status": "disabled", "message": "Auto recovery system is disabled"}
    
    async def stop_auto_recovery(self):
        """–û—Å—Ç–∞–Ω–æ–≤–∫–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è - –û–¢–ö–õ–Æ–ß–ï–ù–û"""
        logger.info("Auto recovery –û–¢–ö–õ–Æ–ß–ï–ù - –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –Ω–µ–≤–æ–∑–º–æ–∂–Ω–∞")
        return {"status": "disabled", "message": "Auto recovery system is disabled"}
    
    def get_auto_recovery_status(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è - –û–¢–ö–õ–Æ–ß–ï–ù–û"""
        return {
            "status": "disabled", 
            "message": "Auto recovery system is disabled",
            "is_running": False,
            "components": {},
            "healthy_components": 0,
            "total_failures": 0
        }
    
    def get_components_status(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ –≤—Å–µ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ - –û–¢–ö–õ–Æ–ß–ï–ù–û"""
        return {
            "status": "disabled",
            "message": "Auto recovery system is disabled",
            "components": {}
        }
    
    def get_recovery_history(self, limit: Optional[int] = None) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è - –û–¢–ö–õ–Æ–ß–ï–ù–û"""
        return []
    
    async def force_component_recovery(self, component_name: str) -> Dict[str, Any]:
        """–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞ - –û–¢–ö–õ–Æ–ß–ï–ù–û"""
        return {
            "status": "disabled",
            "message": "Auto recovery system is disabled",
            "component_name": component_name,
            "success": False
        }
    
    def get_proactive_execution_results(self, limit: Optional[int] = None) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –ø—Ä–æ–∞–∫—Ç–∏–≤–Ω—ã—Ö —Ü–µ–ª–µ–π"""
        if not hasattr(self, 'proactive_timer'):
            return []
        
        results = self.proactive_timer.get_execution_results(limit)
        return [
            {
                "goal_id": result.goal_id,
                "success": result.success,
                "message": result.message,
                "execution_time": result.execution_time,
                "timestamp": result.timestamp.isoformat(),
                "data": result.data
            }
            for result in results
        ]
    
    # ==================== –ú–ï–ñ–£–†–û–í–ù–ï–í–´–ï –°–°–´–õ–ö–ò ====================
    
    async def _track_interlevel_reference(self, source_id: str, target_id: str, 
                                        source_level: str, target_level: str, 
                                        user_id: str) -> None:
        """–û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –º–µ–∂—É—Ä–æ–≤–Ω–µ–≤–æ–π —Å—Å—ã–ª–∫–∏ –º–µ–∂–¥—É —ç–ª–µ–º–µ–Ω—Ç–∞–º–∏ –ø–∞–º—è—Ç–∏"""
        try:
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Å—ã–ª–∫—É –≤ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤
            reference_data = {
                "source_id": source_id,
                "target_id": target_id,
                "source_level": source_level,
                "target_level": target_level,
                "user_id": user_id,
                "created_at": datetime.now().isoformat()
            }
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Å—Å—ã–ª–∫—É –≤ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Ü–µ–ª–µ–≤–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞
            await self._add_reference_to_metadata(target_id, target_level, user_id, reference_data)
            
            logger.debug(f"Interlevel reference tracked: {source_level}:{source_id} -> {target_level}:{target_id}")
            
        except Exception as e:
            logger.warning(f"Failed to track interlevel reference: {e}")
    
    async def _add_reference_to_metadata(self, item_id: str, level: str, user_id: str, 
                                       reference_data: Dict[str, Any]) -> None:
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Å—Å—ã–ª–∫–∏ –≤ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç–∞ –ø–∞–º—è—Ç–∏"""
        try:
            # –ü–æ–ª—É—á–∞–µ–º —ç–ª–µ–º–µ–Ω—Ç –ø–æ ID –∏ —É—Ä–æ–≤–Ω—é
            item = await self._get_memory_item_by_id(item_id, level, user_id)
            if not item:
                return
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Å—Å—ã–ª–∫—É –≤ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            if not hasattr(item, 'metadata') or not item.metadata:
                item.metadata = {}
            
            if 'interlevel_references' not in item.metadata:
                item.metadata['interlevel_references'] = []
            
            item.metadata['interlevel_references'].append(reference_data)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π —ç–ª–µ–º–µ–Ω—Ç
            await self._update_memory_item_metadata(item_id, level, user_id, item.metadata)
            
        except Exception as e:
            logger.warning(f"Failed to add reference to metadata: {e}")
    
    async def _get_memory_item_by_id(self, item_id: str, level: str, user_id: str) -> Optional[Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —ç–ª–µ–º–µ–Ω—Ç–∞ –ø–∞–º—è—Ç–∏ –ø–æ ID –∏ —É—Ä–æ–≤–Ω—é"""
        try:
            if level == "working":
                return await self.working_memory.get_memory_by_id(item_id, user_id)
            elif level == "short_term":
                return await self.short_term_memory.get_memory_by_id(item_id, user_id)
            elif level == "episodic":
                return await self.episodic_memory.get_memory_by_id(item_id, user_id)
            elif level == "semantic":
                return await self.semantic_memory.get_memory_by_id(item_id, user_id)
            elif level == "graph":
                return await self.graph_memory.get_node_by_id(item_id, user_id)
            elif level == "procedural":
                return await self.procedural_memory.get_memory_by_id(item_id, user_id)
            
            return None
            
        except Exception as e:
            logger.warning(f"Failed to get memory item by ID: {e}")
            return None
    
    async def _update_memory_item_metadata(self, item_id: str, level: str, user_id: str, 
                                         metadata: Dict[str, Any]) -> None:
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–∞ –ø–∞–º—è—Ç–∏"""
        try:
            if level == "working":
                await self.working_memory.update_metadata(item_id, user_id, metadata)
            elif level == "short_term":
                await self.short_term_memory.update_metadata(item_id, user_id, metadata)
            elif level == "episodic":
                await self.episodic_memory.update_metadata(item_id, user_id, metadata)
            elif level == "semantic":
                await self.semantic_memory.update_metadata(item_id, user_id, metadata)
            elif level == "graph":
                await self.graph_memory.update_node_metadata(item_id, user_id, metadata)
            elif level == "procedural":
                await self.procedural_memory.update_metadata(item_id, user_id, metadata)
                
        except Exception as e:
            logger.warning(f"Failed to update memory item metadata: {e}")
    
    async def _check_interlevel_duplicates(self, content: str, user_id: str, 
                                         exclude_level: Optional[str] = None) -> List[Dict[str, Any]]:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –º–µ–∂–¥—É —É—Ä–æ–≤–Ω—è–º–∏ –ø–∞–º—è—Ç–∏"""
        try:
            duplicates = []
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Å–µ —É—Ä–æ–≤–Ω–∏ –∫—Ä–æ–º–µ –∏—Å–∫–ª—é—á–µ–Ω–Ω–æ–≥–æ
            levels_to_check = ["working", "short_term", "episodic", "semantic", "procedural"]
            if exclude_level:
                levels_to_check = [level for level in levels_to_check if level != exclude_level]
            
            for level in levels_to_check:
                # –ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–µ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –Ω–∞ —É—Ä–æ–≤–Ω–µ
                similar_items = await self._find_similar_content_on_level(
                    content, level, user_id, similarity_threshold=0.8
                )
                
                for item in similar_items:
                    duplicates.append({
                        "level": level,
                        "item_id": item.get("id"),
                        "similarity": item.get("similarity", 0.0),
                        "content": item.get("content", "")[:100] + "..."
                    })
            
            return duplicates
            
        except Exception as e:
            logger.warning(f"Failed to check interlevel duplicates: {e}")
            return []
    
    async def _find_similar_content_on_level(self, content: str, level: str, user_id: str, 
                                           similarity_threshold: float = 0.8) -> List[Dict[str, Any]]:
        """–ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–µ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º —É—Ä–æ–≤–Ω–µ –ø–∞–º—è—Ç–∏"""
        try:
            # –ü—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–ª–∏–Ω—ã –∏ –ø–µ—Ä–≤—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤
            # –í —Ä–µ–∞–ª—å–Ω–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –∑–¥–µ—Å—å –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º
            similar_items = []
            
            # –ü–æ–ª—É—á–∞–µ–º —ç–ª–µ–º–µ–Ω—Ç—ã —É—Ä–æ–≤–Ω—è –∏—Å–ø–æ–ª—å–∑—É—è —Ä–µ–∞–ª—å–Ω—ã–µ –º–µ—Ç–æ–¥—ã
            if level == "working":
                items = await self.working_memory.get_active_context(user_id, limit=100)
            elif level == "short_term":
                items = await self.short_term_memory.get_recent_events(user_id, hours=168, limit=100)  # 7 –¥–Ω–µ–π
            elif level == "episodic":
                items = await self.episodic_memory.get_significant_events(user_id, days=30, limit=100)
            elif level == "semantic":
                # –î–ª—è semantic –∏—Å–ø–æ–ª—å–∑—É–µ–º get_knowledge_by_category —Å –æ–±—â–∏–º–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º–∏
                items = []
                categories = ["general", "knowledge", "fact", "concept", "information"]
                for category in categories:
                    try:
                        category_items = await self.semantic_memory.get_knowledge_by_category(user_id, category, limit=20)
                        items.extend(category_items)
                    except Exception:
                        continue
            elif level == "procedural":
                # –î–ª—è procedural –∏—Å–ø–æ–ª—å–∑—É–µ–º get_skills_by_type —Å –æ–±—â–∏–º–∏ —Ç–∏–ø–∞–º–∏
                items = []
                skill_types = ["general", "skill", "procedure", "knowledge", "ability"]
                for skill_type in skill_types:
                    try:
                        skill_items = await self.procedural_memory.get_skills_by_type(user_id, skill_type, limit=20)
                        items.extend(skill_items)
                    except Exception:
                        continue
            else:
                return similar_items
            
            # –ü—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ö–æ–∂–µ—Å—Ç–∏
            for item in items:
                if hasattr(item, 'content') and item.content:
                    similarity = self._calculate_content_similarity(content, item.content)
                    if similarity >= similarity_threshold:
                        similar_items.append({
                            "id": getattr(item, 'id', None),
                            "content": item.content,
                            "similarity": similarity
                        })
            
            return similar_items
            
        except Exception as e:
            logger.warning(f"Failed to find similar content on level {level}: {e}")
            return []
    
    def _calculate_content_similarity(self, content1: str, content2: str) -> float:
        """–ü—Ä–æ—Å—Ç–æ–π —Ä–∞—Å—á–µ—Ç —Å—Ö–æ–∂–µ—Å—Ç–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        try:
            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
            text1 = content1.lower().strip()
            text2 = content2.lower().strip()
            
            # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç—ã –∏–¥–µ–Ω—Ç–∏—á–Ω—ã
            if text1 == text2:
                return 1.0
            
            # –ï—Å–ª–∏ –æ–¥–∏–Ω —Ç–µ–∫—Å—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç –¥—Ä—É–≥–æ–π
            if text1 in text2 or text2 in text1:
                return 0.9
            
            # –ü—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–ª–∏–Ω—ã –∏ –æ–±—â–∏—Ö —Å–ª–æ–≤
            words1 = set(text1.split())
            words2 = set(text2.split())
            
            if not words1 or not words2:
                return 0.0
            
            intersection = len(words1.intersection(words2))
            union = len(words1.union(words2))
            
            return intersection / union if union > 0 else 0.0
            
        except Exception as e:
            logger.warning(f"Failed to calculate content similarity: {e}")
            return 0.0
    
    async def get_interlevel_references(self, item_id: str, level: str, user_id: str) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö –º–µ–∂—É—Ä–æ–≤–Ω–µ–≤—ã—Ö —Å—Å—ã–ª–æ–∫ –¥–ª—è —ç–ª–µ–º–µ–Ω—Ç–∞"""
        try:
            item = await self._get_memory_item_by_id(item_id, level, user_id)
            if not item or not hasattr(item, 'metadata') or not item.metadata:
                return []
            
            return item.metadata.get('interlevel_references', [])
            
        except Exception as e:
            logger.warning(f"Failed to get interlevel references: {e}")
            return []
    
    async def consolidate_with_interlevel_tracking(self, user_id: str) -> Dict[str, Any]:
        """–ö–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏—è —Å –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ–º –º–µ–∂—É—Ä–æ–≤–Ω–µ–≤—ã—Ö —Å—Å—ã–ª–æ–∫"""
        try:
            logger.info(f"Starting consolidation with interlevel tracking for user {user_id}")
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º –æ–±—ã—á–Ω—É—é –∫–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏—é
            consolidation_result = await self.consolidate_memories(user_id)
            
            # –û—Ç—Å–ª–µ–∂–∏–≤–∞–µ–º –º–µ–∂—É—Ä–æ–≤–Ω–µ–≤—ã–µ —Å—Å—ã–ª–∫–∏ –¥–ª—è –ø–µ—Ä–µ–º–µ—â–µ–Ω–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤
            interlevel_tracking = {
                "working_to_short_term": 0,
                "short_term_to_episodic": 0,
                "episodic_to_semantic": 0,
                "total_references_created": 0
            }
            
            # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ª–æ–≥–∏–∫—É –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è —Å—Å—ã–ª–æ–∫
            # –ø—Ä–∏ –ø–µ—Ä–µ–º–µ—â–µ–Ω–∏–∏ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –º–µ–∂–¥—É —É—Ä–æ–≤–Ω—è–º–∏
            
            consolidation_result["interlevel_tracking"] = interlevel_tracking.get("total_references_created", 0)
            
            logger.info(f"Consolidation with interlevel tracking completed for user {user_id}")
            return consolidation_result
            
        except Exception as e:
            logger.error(f"Failed to consolidate with interlevel tracking: {e}")
            return {"error": str(e)}
    
    def _is_monitoring_working(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞"""
        try:
            # –¢–µ—Å—Ç–∏—Ä—É–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
            inc("monitoring_test", 1)
            set_gauge("monitoring_test_gauge", 1.0)
            record_event("monitoring_test", {"test": True})
            logger.debug("‚úÖ Monitoring test passed - all functions working")
            return True
        except Exception as e:
            logger.error(f"üö® CRITICAL: Monitoring test failed: {e}")
            logger.error("üö® Monitoring is using dummy functions")
            return False
    
    def get_monitoring_status(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞"""
        try:
            return {
                "monitoring_working": self._is_monitoring_working(),
                "memory_monitor_available": self.memory_monitor is not None,
                "memory_cleanup_available": self.memory_cleanup is not None,
                "metrics_available": True,  # –ë—É–¥–µ—Ç –ø—Ä–æ–≤–µ—Ä–µ–Ω–æ –≤ _is_monitoring_working
                "last_test_time": datetime.utcnow().isoformat()
            }
        except Exception as e:
            logger.error(f"Failed to get monitoring status: {e}")
            return {
                "monitoring_working": False,
                "error": str(e)
            }
