"""
LM Studio Provider Enhanced –¥–ª—è AIRI Memory System
–£–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º, –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –ø—Ä–æ–º–ø—Ç–æ–≤ –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–æ–º
"""

import asyncio
import json
import hashlib
import time
from typing import Dict, List, Optional, Any, Tuple
import httpx
from loguru import logger
import yaml
from pathlib import Path
from datetime import datetime, timedelta
import pickle
import os
import sys

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –º–æ–¥—É–ª—è–º MemorySystem
sys.path.append(str(Path(__file__).parent.parent))
from emotion_formatter import EmotionFormatter, EmotionData, FormattedEmotion

class LMStudioProviderEnhanced:
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å LM Studio —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
    
    def __init__(self, config_path: str = "config/lm_studio_config.yaml"):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞ LM Studio"""
        self.config = self._load_config(config_path)
        self.client = httpx.AsyncClient(
            timeout=self.config.get("timeout", 120),
            base_url=self.config["base_url"]
        )
        
        # –ö—ç—à –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        self.cache = {}
        self.cache_file = "lm_studio_cache.pkl"
        self.cache_ttl = self.config.get("cache_ttl", 3600)  # 1 —á–∞—Å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.stats = {
            "total_requests": 0,
            "cache_hits": 0,
            "cache_misses": 0,
            "total_time": 0.0,
            "average_time": 0.0
        }
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫—ç—à –∏–∑ —Ñ–∞–π–ª–∞
        self._load_cache()
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Ñ–æ—Ä–º–∞—Ç—Ç–µ—Ä —ç–º–æ—Ü–∏–π
        self.emotion_formatter = EmotionFormatter()
        
        logger.info(f"LM Studio Provider Enhanced –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω: {self.config['base_url']}")
        logger.info(f"–ö—ç—à TTL: {self.cache_ttl} —Å–µ–∫—É–Ω–¥")
        logger.info(f"–§–æ—Ä–º–∞—Ç—Ç–µ—Ä —ç–º–æ—Ü–∏–π: –≥–æ—Ç–æ–≤")
    
    def _load_config(self, config_path: str) -> Dict[str, Any]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        if isinstance(config_path, dict):
            return config_path
            
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
                # –î–æ–±–∞–≤–ª—è–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è
                config.setdefault("cache_ttl", 3600)  # 1 —á–∞—Å
                config.setdefault("cache_enabled", True)
                config.setdefault("prompt_optimization", True)
                return config
        except FileNotFoundError:
            logger.warning(f"–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π —Ñ–∞–π–ª {config_path} –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é")
            return {
                "base_url": "http://localhost:11434/v1",
                "model": "gemma2:2b",
                "max_tokens": 2048,
                "temperature": 0.7,
                "timeout": 120,
                "retry_attempts": 3,
                "cache_ttl": 3600,
                "cache_enabled": True,
                "prompt_optimization": True
            }
    
    def _load_cache(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∫—ç—à–∞ –∏–∑ —Ñ–∞–π–ª–∞"""
        try:
            if os.path.exists(self.cache_file):
                with open(self.cache_file, 'rb') as f:
                    self.cache = pickle.load(f)
                logger.info(f"–ö—ç—à –∑–∞–≥—Ä—É–∂–µ–Ω: {len(self.cache)} –∑–∞–ø–∏—Å–µ–π")
            else:
                self.cache = {}
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫—ç—à–∞: {e}")
            self.cache = {}
    
    def _save_cache(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫—ç—à–∞ –≤ —Ñ–∞–π–ª"""
        try:
            with open(self.cache_file, 'wb') as f:
                pickle.dump(self.cache, f)
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∫—ç—à–∞: {e}")
    
    def _get_cache_key(self, prompt: str, max_tokens: int, temperature: float) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–ª—é—á–∞ –∫—ç—à–∞"""
        content = f"{prompt}|{max_tokens}|{temperature}"
        return hashlib.md5(content.encode()).hexdigest()
    
    def _is_cache_valid(self, cache_entry: Dict) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏ –∑–∞–ø–∏—Å–∏ –∫—ç—à–∞"""
        if not self.config.get("cache_enabled", True):
            return False
        
        timestamp = cache_entry.get("timestamp", 0)
        return time.time() - timestamp < self.cache_ttl
    
    def _optimize_prompt(self, prompt: str) -> str:
        """–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏"""
        if not self.config.get("prompt_optimization", True):
            return prompt
        
        # –£–±–∏—Ä–∞–µ–º –∏–∑–±—ã—Ç–æ—á–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã
        prompt = " ".join(prompt.split())
        
        # –°–æ–∫—Ä–∞—â–∞–µ–º –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è —Ñ—Ä–∞–∑—ã
        lines = prompt.split('\n')
        optimized_lines = []
        seen_lines = set()
        
        for line in lines:
            line = line.strip()
            if line and line not in seen_lines:
                optimized_lines.append(line)
                seen_lines.add(line)
        
        optimized_prompt = '\n'.join(optimized_lines)
        
        # –õ–æ–≥–∏—Ä—É–µ–º –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é
        if len(optimized_prompt) < len(prompt):
            reduction = len(prompt) - len(optimized_prompt)
            logger.info(f"–ü—Ä–æ–º–ø—Ç –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω: -{reduction} —Å–∏–º–≤–æ–ª–æ–≤ ({reduction/len(prompt)*100:.1f}%)")
        
        return optimized_prompt
    
    async def generate_text(
        self, 
        prompt: str, 
        max_tokens: Optional[int] = None,
        temperature: Optional[float] = None,
        use_cache: bool = True
    ) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
        start_time = time.time()
        self.stats["total_requests"] += 1
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        max_tokens = max_tokens or self.config.get("max_tokens", 512)
        temperature = temperature or self.config.get("temperature", 0.7)
        
        # –û–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç
        optimized_prompt = self._optimize_prompt(prompt)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
        if use_cache and self.config.get("cache_enabled", True):
            cache_key = self._get_cache_key(optimized_prompt, max_tokens, temperature)
            
            if cache_key in self.cache:
                cache_entry = self.cache[cache_key]
                if self._is_cache_valid(cache_entry):
                    self.stats["cache_hits"] += 1
                    processing_time = time.time() - start_time
                    self.stats["total_time"] += processing_time
                    self.stats["average_time"] = self.stats["total_time"] / self.stats["total_requests"]
                    
                    logger.info(f"–ö—ç—à HIT: {processing_time:.3f}—Å (–∫–ª—é—á: {cache_key[:8]}...)")
                    return cache_entry["response"]
                else:
                    # –£–¥–∞–ª—è–µ–º —É—Å—Ç–∞—Ä–µ–≤—à—É—é –∑–∞–ø–∏—Å—å
                    del self.cache[cache_key]
        
        self.stats["cache_misses"] += 1
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç
        try:
            response = await self._generate_with_retry(optimized_prompt, max_tokens, temperature)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à
            if use_cache and self.config.get("cache_enabled", True):
                cache_key = self._get_cache_key(optimized_prompt, max_tokens, temperature)
                self.cache[cache_key] = {
                    "response": response,
                    "timestamp": time.time(),
                    "prompt_length": len(optimized_prompt),
                    "max_tokens": max_tokens,
                    "temperature": temperature
                }
                self._save_cache()
            
            processing_time = time.time() - start_time
            self.stats["total_time"] += processing_time
            self.stats["average_time"] = self.stats["total_time"] / self.stats["total_requests"]
            
            logger.info(f"–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {processing_time:.3f}—Å")
            return response
            
        except Exception as e:
            processing_time = time.time() - start_time
            logger.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e} ({processing_time:.3f}—Å)")
            raise
    
    async def _generate_with_retry(
        self, 
        prompt: str, 
        max_tokens: int, 
        temperature: float
    ) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏"""
        retry_attempts = self.config.get("retry_attempts", 3)
        
        for attempt in range(retry_attempts):
            try:
                payload = {
                    "model": self.config["model"],
                    "messages": [
                        {"role": "user", "content": prompt}
                    ],
                    "max_tokens": max_tokens,
                    "temperature": temperature,
                    "stream": False
                }
                
                response = await self.client.post("/chat/completions", json=payload)
                
                if response.status_code == 200:
                    result = response.json()
                    return result["choices"][0]["message"]["content"].strip()
                else:
                    logger.warning(f"HTTP {response.status_code}: {response.text}")
                    if attempt < retry_attempts - 1:
                        await asyncio.sleep(2 ** attempt)  # –≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞
                        continue
                    else:
                        raise Exception(f"HTTP {response.status_code}: {response.text}")
                        
            except httpx.TimeoutException:
                logger.warning(f"–¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –æ–±—Ä–∞—â–µ–Ω–∏–∏ –∫ Ollama (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{retry_attempts})")
                if attempt < retry_attempts - 1:
                    await asyncio.sleep(2 ** attempt)
                    continue
                else:
                    raise Exception("–¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –æ–±—Ä–∞—â–µ–Ω–∏–∏ –∫ LM Studio")
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞—â–µ–Ω–∏–∏ –∫ Ollama (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{retry_attempts}): {e}")
                if attempt < retry_attempts - 1:
                    await asyncio.sleep(2 ** attempt)
                    continue
                else:
                    raise
    
    async def analyze_memory_content(self, content: str, context: Optional[str] = None) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –ø–∞–º—è—Ç–∏ —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
        # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        prompt = self._create_analysis_prompt(content, context)
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        analysis_config = self.config.get("tasks", {}).get("analysis", {})
        max_tokens = analysis_config.get("max_tokens", 256)
        temperature = analysis_config.get("temperature", 0.3)
        
        try:
            response_text = await self.generate_text(prompt, max_tokens, temperature)
            return self._parse_analysis_response(response_text)
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ: {e}")
            return self._get_fallback_analysis(content)
    
    def _create_analysis_prompt(self, content: str, context: Optional[str] = None) -> str:
        """–°–æ–∑–¥–∞–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞"""
        base_prompt = f"""–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å–ª–µ–¥—É—é—â–∏–π —Ç–µ–∫—Å—Ç –∏ –≤–µ—Ä–Ω–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON:

–¢–µ–∫—Å—Ç: "{content[:500]}{'...' if len(content) > 500 else ''}"

–í–µ—Ä–Ω–∏ JSON —Å –ø–æ–ª—è–º–∏:
- summary: –∫—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ (–¥–æ 50 —Å–ª–æ–≤)
- keywords: –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (–º–∞—Å—Å–∏–≤ —Å—Ç—Ä–æ–∫)
- type: —Ç–∏–ø —Å–æ–±—ã—Ç–∏—è (conversation, question, statement, etc.)
- importance: –≤–∞–∂–Ω–æ—Å—Ç—å (low, medium, high)
- entities: –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏ (–º–∞—Å—Å–∏–≤ —Å—Ç—Ä–æ–∫)
- sentiment: —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è –æ–∫—Ä–∞—Å–∫–∞ (positive, negative, neutral)"""
        
        if context:
            base_prompt += f"\n\n–ö–æ–Ω—Ç–µ–∫—Å—Ç: {context[:200]}{'...' if len(context) > 200 else ''}"
        
        return base_prompt
    
    def _parse_analysis_response(self, response_text: str) -> Dict[str, Any]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –æ—Ç–≤–µ—Ç–∞ –∞–Ω–∞–ª–∏–∑–∞"""
        try:
            # –ò—â–µ–º JSON –≤ –æ—Ç–≤–µ—Ç–µ
            start_idx = response_text.find('{')
            end_idx = response_text.rfind('}') + 1
            
            if start_idx != -1 and end_idx > start_idx:
                json_str = response_text[start_idx:end_idx]
                return json.loads(json_str)
            else:
                raise ValueError("JSON –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –æ—Ç–≤–µ—Ç–µ")
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –æ—Ç–≤–µ—Ç–∞: {e}")
            return self._get_fallback_analysis(response_text)
    
    def _get_fallback_analysis(self, content: str) -> Dict[str, Any]:
        """Fallback –∞–Ω–∞–ª–∏–∑ –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö"""
        return {
            "summary": content[:100] + "..." if len(content) > 100 else content,
            "keywords": content.split()[:5],
            "type": "unknown",
            "importance": "medium",
            "entities": [],
            "sentiment": "neutral"
        }
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∫—ç—à–∞"""
        valid_entries = sum(1 for entry in self.cache.values() if self._is_cache_valid(entry))
        total_entries = len(self.cache)
        
        return {
            "total_entries": total_entries,
            "valid_entries": valid_entries,
            "expired_entries": total_entries - valid_entries,
            "cache_hit_rate": self.stats["cache_hits"] / max(self.stats["total_requests"], 1),
            "average_response_time": self.stats["average_time"],
            "total_requests": self.stats["total_requests"],
            "cache_hits": self.stats["cache_hits"],
            "cache_misses": self.stats["cache_misses"]
        }
    
    def create_emotion_enhanced_prompt(
        self, 
        base_prompt: str, 
        emotion_data: Optional[Dict[str, Any]] = None
    ) -> str:
        """–°–æ–∑–¥–∞–µ—Ç –ø—Ä–æ–º–ø—Ç —Å —É—á–µ—Ç–æ–º —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
        
        if not emotion_data:
            return base_prompt
        
        try:
            # –°–æ–∑–¥–∞–µ–º EmotionData –∏–∑ –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            emotion_obj = EmotionData(
                primary_emotion=emotion_data.get('primary_emotion', '–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–∞—è'),
                primary_confidence=emotion_data.get('primary_confidence', 0.5),
                secondary_emotion=emotion_data.get('secondary_emotion'),
                secondary_confidence=emotion_data.get('secondary_confidence'),
                tertiary_emotion=emotion_data.get('tertiary_emotion'),
                tertiary_confidence=emotion_data.get('tertiary_confidence'),
                consistency=emotion_data.get('consistency', 'high'),
                dominant_source=emotion_data.get('dominant_source', 'voice'),
                validation_applied=emotion_data.get('validation_applied', False)
            )
            
            # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —ç–º–æ—Ü–∏–∏
            formatted_emotion = self.emotion_formatter.format_emotions_for_ai(emotion_obj)
            
            # –°–æ–∑–¥–∞–µ–º –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π –ø—Ä–æ–º–ø—Ç
            enhanced_prompt = self.emotion_formatter.create_adaptive_prompt(formatted_emotion, base_prompt)
            
            logger.info(f"–°–æ–∑–¥–∞–Ω —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ-—É—Å–∏–ª–µ–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç: {len(enhanced_prompt)} —Å–∏–º–≤–æ–ª–æ–≤")
            logger.info(f"–°–ª–æ–∂–Ω–æ—Å—Ç—å —ç–º–æ—Ü–∏–π: {formatted_emotion.complexity.value}")
            
            return enhanced_prompt
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞: {e}")
            return base_prompt
    
    async def generate_text_with_emotions(
        self, 
        prompt: str, 
        emotion_data: Optional[Dict[str, Any]] = None,
        max_tokens: int = 500,
        temperature: float = 0.7
    ) -> Dict[str, Any]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç —Å —É—á–µ—Ç–æ–º —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
        
        # –°–æ–∑–¥–∞–µ–º —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ-—É—Å–∏–ª–µ–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç
        enhanced_prompt = self.create_emotion_enhanced_prompt(prompt, emotion_data)
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç
        result = await self.generate_text(
            prompt=enhanced_prompt,
            max_tokens=max_tokens,
            temperature=temperature
        )
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± —ç–º–æ—Ü–∏—è—Ö –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        if emotion_data:
            result["emotion_context"] = {
                "original_prompt": prompt,
                "enhanced_prompt": enhanced_prompt,
                "emotion_data": emotion_data,
                "prompt_enhancement": len(enhanced_prompt) - len(prompt)
            }
        
        return result
    
    def clear_cache(self):
        """–û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞"""
        self.cache.clear()
        if os.path.exists(self.cache_file):
            os.remove(self.cache_file)
        logger.info("–ö—ç—à –æ—á–∏—â–µ–Ω")
    
    def cleanup_expired_cache(self):
        """–û—á–∏—Å—Ç–∫–∞ —É—Å—Ç–∞—Ä–µ–≤—à–∏—Ö –∑–∞–ø–∏—Å–µ–π –∫—ç—à–∞"""
        expired_keys = []
        for key, entry in self.cache.items():
            if not self._is_cache_valid(entry):
                expired_keys.append(key)
        
        for key in expired_keys:
            del self.cache[key]
        
        if expired_keys:
            self._save_cache()
            logger.info(f"–£–¥–∞–ª–µ–Ω–æ {len(expired_keys)} —É—Å—Ç–∞—Ä–µ–≤—à–∏—Ö –∑–∞–ø–∏—Å–µ–π –∫—ç—à–∞")
    
    async def close(self):
        """–ó–∞–∫—Ä—ã—Ç–∏–µ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞"""
        await self.client.aclose()
        self._save_cache()
        logger.info("LM Studio Provider Enhanced –∑–∞–∫—Ä—ã—Ç")

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
async def test_enhanced_provider():
    """–¢–µ—Å—Ç —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞"""
    print("üß™ –¢–µ—Å—Ç–∏—Ä—É–µ–º LM Studio Provider Enhanced...")
    
    provider = LMStudioProviderEnhanced()
    
    # –¢–µ—Å—Ç–æ–≤—ã–π –ø—Ä–æ–º–ø—Ç
    test_prompt = "–ü—Ä–∏–≤–µ—Ç, –∫–∞–∫ –¥–µ–ª–∞? –†–∞—Å—Å–∫–∞–∂–∏ —á—Ç–æ-–Ω–∏–±—É–¥—å –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ–µ."
    
    try:
        # –ü–µ—Ä–≤—ã–π –∑–∞–ø—Ä–æ—Å (–∫—ç—à miss)
        print("üîÑ –ü–µ—Ä–≤—ã–π –∑–∞–ø—Ä–æ—Å (–∫—ç—à miss)...")
        start_time = time.time()
        response1 = await provider.generate_text(test_prompt, max_tokens=100, temperature=0.7)
        time1 = time.time() - start_time
        print(f"   –í—Ä–µ–º—è: {time1:.2f}—Å")
        print(f"   –û—Ç–≤–µ—Ç: {response1[:100]}...")
        
        # –í—Ç–æ—Ä–æ–π –∑–∞–ø—Ä–æ—Å (–∫—ç—à hit)
        print("\nüîÑ –í—Ç–æ—Ä–æ–π –∑–∞–ø—Ä–æ—Å (–∫—ç—à hit)...")
        start_time = time.time()
        response2 = await provider.generate_text(test_prompt, max_tokens=100, temperature=0.7)
        time2 = time.time() - start_time
        print(f"   –í—Ä–µ–º—è: {time2:.2f}—Å")
        print(f"   –û—Ç–≤–µ—Ç: {response2[:100]}...")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –æ—Ç–≤–µ—Ç—ã –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ
        if response1 == response2:
            print("‚úÖ –û—Ç–≤–µ—Ç—ã –∏–¥–µ–Ω—Ç–∏—á–Ω—ã")
        else:
            print("‚ùå –û—Ç–≤–µ—Ç—ã —Ä–∞–∑–ª–∏—á–∞—é—Ç—Å—è")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        stats = provider.get_cache_stats()
        print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
        print(f"   –í—Å–µ–≥–æ –∑–∞–ø—Ä–æ—Å–æ–≤: {stats['total_requests']}")
        print(f"   –ö—ç—à –ø–æ–ø–∞–¥–∞–Ω–∏—è: {stats['cache_hits']}")
        print(f"   –ö—ç—à –ø—Ä–æ–º–∞—Ö–∏: {stats['cache_misses']}")
        print(f"   Hit rate: {stats['cache_hit_rate']:.1%}")
        print(f"   –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è: {stats['average_response_time']:.2f}—Å")
        print(f"   –£—Å–∫–æ—Ä–µ–Ω–∏–µ: {time1/time2:.1f}x")
        
        await provider.close()
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")
        await provider.close()

if __name__ == "__main__":
    asyncio.run(test_enhanced_provider())
